{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "meteor = evaluate.load('meteor')\n",
    "bleu = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score computing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_hf(founds, answer):\n",
    "    # A kapott válaszokat egy sztringgé alakítjuk a kiértékeléshez\n",
    "    predicted_text = \" \".join(founds)\n",
    "    reference_text = \" \".join(answer)\n",
    "\n",
    "    # Rouge\n",
    "    rouge_scores = rouge.compute(predictions=[predicted_text], references=[reference_text])\n",
    "\n",
    "    # Meteor\n",
    "    meteor_score = meteor.compute(predictions=[predicted_text], references=[reference_text])\n",
    "\n",
    "    # Bleu\n",
    "    bleu_score = bleu.compute(predictions=[predicted_text], references=[reference_text])\n",
    "\n",
    "    return rouge_scores, meteor_score, bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(resp):\n",
    "    return resp.split(\"\\n---------------------\\n\")[1].split(\"\\n\\n\")\n",
    "\n",
    "def tb_query(index, df, top_k, size, min_k=1):\n",
    "    for k in range(min_k, top_k+1):\n",
    "        query_engine = index.as_query_engine(similarity_top_k=k)\n",
    "\n",
    "        resps = []\n",
    "        for q in tqdm(df[\"question\"].values, desc=f\"({size} token, k = {k}) Get responses\", leave=False):\n",
    "            resps.append(get_response(query_engine.query(q).response))\n",
    "\n",
    "        df[f\"answer_tb_{size}_k{k}\"] = resps\n",
    "    \n",
    "    answer_df = df.loc[:, df.columns.str.contains('answer', case=False)]\n",
    "\n",
    "    return answer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_tb(dataframe, size):\n",
    "    num_columns = dataframe.shape[1] - 1\n",
    "\n",
    "    res_df = pd.DataFrame()\n",
    "\n",
    "    for k in range(1, int(num_columns) + 1):\n",
    "\n",
    "        rouge_scores = [0] * len(dataframe)\n",
    "        meteor_scores = [0] * len(dataframe)\n",
    "        bleu_scores = [0] * len(dataframe)\n",
    "        answers = [0] * len(dataframe)\n",
    "\n",
    "        for i in tqdm(range(0,len(dataframe)), desc=f\"(k = {k}) Computing {size} scores\"):\n",
    "            rouge_score, meteor_score, bleu_score = evaluate_with_hf(\n",
    "                dataframe[f\"answer_tb_{size}_k{k}\"].values[i],\n",
    "                dataframe[\"answer\"].values[i])\n",
    "            \n",
    "            rouge_scores[i] = rouge_score\n",
    "            meteor_scores[i] = meteor_score\n",
    "            bleu_scores[i] = bleu_score\n",
    "            answers[i] = dataframe[f\"answer_tb_{size}_k{k}\"].values[i],\n",
    "\n",
    "        res_df[f\"rouge_score_tb_{size}_k{k}\"] = rouge_scores\n",
    "        res_df[f\"meteor_score_tb_{size}_k{k}\"] = meteor_scores\n",
    "        res_df[f\"bleu_score_tb_{size}_k{k}\"] = bleu_scores\n",
    "        res_df[f\"answer_tb_{size}_k{k}\"] = answers\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_scores_tb(dataframe, size):\n",
    "    num_columns = (dataframe.shape[1]) / 4\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for k in range(1, int(num_columns) + 1):\n",
    "        # Rouge átlagok számítása\n",
    "        rouge1_avg = dataframe[f'rouge_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rouge1']).mean()\n",
    "        rouge2_avg = dataframe[f'rouge_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rouge2']).mean()\n",
    "        rougeL_avg = dataframe[f'rouge_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rougeL']).mean()\n",
    "        rougeLsum_avg = dataframe[f'rouge_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rougeLsum']).mean()\n",
    "\n",
    "        # Meteor átlag\n",
    "        meteor_avg = dataframe[f'meteor_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['meteor']).mean()\n",
    "\n",
    "        # Bleu értékek átlagolása\n",
    "        bleu_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['bleu']).mean()\n",
    "\n",
    "        # Precision értékek átlagolása a precisions listából\n",
    "        precision1_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][0]).mean()\n",
    "        precision2_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][1]).mean()\n",
    "        precision3_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][2]).mean()\n",
    "        precision4_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][3]).mean()\n",
    "\n",
    "        # Egyéb Bleu metrikák átlagolása\n",
    "        brevity_penalty_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['brevity_penalty']).mean()\n",
    "        length_ratio_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['length_ratio']).mean()\n",
    "        translation_length_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['translation_length']).mean()\n",
    "        reference_length_avg = dataframe[f'bleu_score_tb_{size}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['reference_length']).mean()\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            \"k\": k,\n",
    "            \"rouge1_avg\": rouge1_avg,\n",
    "            \"rouge2_avg\": rouge2_avg,\n",
    "            \"rougeL_avg\": rougeL_avg,\n",
    "            \"rougeLsum_avg\": rougeLsum_avg,\n",
    "            \"meteor_avg\": meteor_avg,\n",
    "            \"bleu_avg\": bleu_avg,\n",
    "            \"precision1_avg\": precision1_avg,\n",
    "            \"precision2_avg\": precision2_avg,\n",
    "            \"precision3_avg\": precision3_avg,\n",
    "            \"precision4_avg\": precision4_avg,\n",
    "            \"brevity_penalty_avg\": brevity_penalty_avg,\n",
    "            \"length_ratio_avg\": length_ratio_avg,\n",
    "            \"translation_length_avg\": translation_length_avg,\n",
    "            \"reference_length_avg\": reference_length_avg\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(resp):\n",
    "    return resp.split(\"\\n---------------------\\n\")[1].split(\"\\n\\n\")\n",
    "\n",
    "def sb_query(index, df, top_k, min_k=1):\n",
    "    for k in range(min_k, top_k+1):\n",
    "        query_engine = index.as_query_engine(similarity_top_k=k)\n",
    "\n",
    "        resps = []\n",
    "        for q in tqdm(df[\"question\"].values, desc=f\"(sentence based, k = {k}) Get responses\", leave=False):\n",
    "            resps.append(get_response(query_engine.query(q).response))\n",
    "\n",
    "        df[f\"answer_sb_k{k}\"] = resps\n",
    "    \n",
    "    answer_df = df.loc[:, df.columns.str.contains('answer', case=False)]\n",
    "\n",
    "    return answer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_sb(dataframe):\n",
    "    num_columns = dataframe.shape[1] - 1\n",
    "\n",
    "    res_df = pd.DataFrame()\n",
    "\n",
    "    for k in range(1, int(num_columns) + 1):\n",
    "\n",
    "        rouge_scores = [0] * len(dataframe)\n",
    "        meteor_scores = [0] * len(dataframe)\n",
    "        bleu_scores = [0] * len(dataframe)\n",
    "        answers = [0] * len(dataframe)\n",
    "\n",
    "        for i in tqdm(range(0,len(dataframe)), desc=f\"(k = {k}) Computing scores\"):\n",
    "            rouge_score, meteor_score, bleu_score = evaluate_with_hf(\n",
    "                dataframe[f\"answer_sb_k{k}\"].values[i],\n",
    "                dataframe[\"answer\"].values[i])\n",
    "            \n",
    "            rouge_scores[i] = rouge_score\n",
    "            meteor_scores[i] = meteor_score\n",
    "            bleu_scores[i] = bleu_score\n",
    "            answers[i] = dataframe[f\"answer_sb_k{k}\"].values[i],\n",
    "\n",
    "        res_df[f\"rouge_score_sb_k{k}\"] = rouge_scores\n",
    "        res_df[f\"meteor_score_sb_k{k}\"] = meteor_scores\n",
    "        res_df[f\"bleu_score_sb_k{k}\"] = bleu_scores\n",
    "        res_df[f\"answer_sb_k{k}\"] = answers\n",
    "\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_scores_sb(dataframe):\n",
    "    num_columns = (dataframe.shape[1]) / 4\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for k in range(1, int(num_columns) + 1):\n",
    "        # Rouge átlagok számítása\n",
    "        rouge1_avg = dataframe[f'rouge_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rouge1']).mean()\n",
    "        rouge2_avg = dataframe[f'rouge_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rouge2']).mean()\n",
    "        rougeL_avg = dataframe[f'rouge_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rougeL']).mean()\n",
    "        rougeLsum_avg = dataframe[f'rouge_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rougeLsum']).mean()\n",
    "\n",
    "        # Meteor átlag\n",
    "        meteor_avg = dataframe[f'meteor_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['meteor']).mean()\n",
    "\n",
    "        # Bleu értékek átlagolása\n",
    "        bleu_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['bleu']).mean()\n",
    "\n",
    "        # Precision értékek átlagolása a precisions listából\n",
    "        precision1_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][0]).mean()\n",
    "        precision2_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][1]).mean()\n",
    "        precision3_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][2]).mean()\n",
    "        precision4_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][3]).mean()\n",
    "\n",
    "        # Egyéb Bleu metrikák átlagolása\n",
    "        brevity_penalty_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['brevity_penalty']).mean()\n",
    "        length_ratio_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['length_ratio']).mean()\n",
    "        translation_length_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['translation_length']).mean()\n",
    "        reference_length_avg = dataframe[f'bleu_score_sb_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['reference_length']).mean()\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            \"k\": k,\n",
    "            \"rouge1_avg\": rouge1_avg,\n",
    "            \"rouge2_avg\": rouge2_avg,\n",
    "            \"rougeL_avg\": rougeL_avg,\n",
    "            \"rougeLsum_avg\": rougeLsum_avg,\n",
    "            \"meteor_avg\": meteor_avg,\n",
    "            \"bleu_avg\": bleu_avg,\n",
    "            \"precision1_avg\": precision1_avg,\n",
    "            \"precision2_avg\": precision2_avg,\n",
    "            \"precision3_avg\": precision3_avg,\n",
    "            \"precision4_avg\": precision4_avg,\n",
    "            \"brevity_penalty_avg\": brevity_penalty_avg,\n",
    "            \"length_ratio_avg\": length_ratio_avg,\n",
    "            \"translation_length_avg\": translation_length_avg,\n",
    "            \"reference_length_avg\": reference_length_avg\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_sw(dataframe, window, overlap):\n",
    "    num_columns = dataframe.shape[1] - 1\n",
    "\n",
    "    res_df = pd.DataFrame()\n",
    "\n",
    "    for k in range(1, int(num_columns) + 1):\n",
    "\n",
    "        rouge_scores = [0] * len(dataframe)\n",
    "        meteor_scores = [0] * len(dataframe)\n",
    "        bleu_scores = [0] * len(dataframe)\n",
    "        answers = [0] * len(dataframe)\n",
    "\n",
    "        for i in tqdm(range(0,len(dataframe)), desc=f\"(k = {k}) Computing scores {window}-{overlap}\"):\n",
    "            rouge_score, meteor_score, bleu_score = evaluate_with_hf(\n",
    "                dataframe[f\"answer_sw_{window}_{overlap}_k{k}\"].values[i],\n",
    "                dataframe[\"answer\"].values[i])\n",
    "            \n",
    "            rouge_scores[i] = rouge_score\n",
    "            meteor_scores[i] = meteor_score\n",
    "            bleu_scores[i] = bleu_score\n",
    "            answers[i] = dataframe[f\"answer_sw_{window}_{overlap}_k{k}\"].values[i],\n",
    "\n",
    "        res_df[f\"rouge_score_sw_{window}_{overlap}_k{k}\"] = rouge_scores\n",
    "        res_df[f\"meteor_score_sw_{window}_{overlap}_k{k}\"] = meteor_scores\n",
    "        res_df[f\"bleu_score_sw_{window}_{overlap}_k{k}\"] = bleu_scores\n",
    "        res_df[f\"answer_sw_{window}_{overlap}_k{k}\"] = answers\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_scores_sw(dataframe, window, overlap):\n",
    "    num_columns = (dataframe.shape[1]) / 4\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for k in range(1, int(num_columns) + 1):\n",
    "        # Rouge átlagok számítása\n",
    "        rouge1_avg = dataframe[f'rouge_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rouge1']).mean()\n",
    "        rouge2_avg = dataframe[f'rouge_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rouge2']).mean()\n",
    "        rougeL_avg = dataframe[f'rouge_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rougeL']).mean()\n",
    "        rougeLsum_avg = dataframe[f'rouge_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['rougeLsum']).mean()\n",
    "\n",
    "        # Meteor átlag\n",
    "        meteor_avg = dataframe[f'meteor_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['meteor']).mean()\n",
    "\n",
    "        # Bleu értékek átlagolása\n",
    "        bleu_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['bleu']).mean()\n",
    "\n",
    "        # Precision értékek átlagolása a precisions listából\n",
    "        precision1_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][0]).mean()\n",
    "        precision2_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][1]).mean()\n",
    "        precision3_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][2]).mean()\n",
    "        precision4_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['precisions'][3]).mean()\n",
    "\n",
    "        # Egyéb Bleu metrikák átlagolása\n",
    "        brevity_penalty_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['brevity_penalty']).mean()\n",
    "        length_ratio_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['length_ratio']).mean()\n",
    "        translation_length_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['translation_length']).mean()\n",
    "        reference_length_avg = dataframe[f'bleu_score_sw_{window}_{overlap}_k{k}'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))['reference_length']).mean()\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            \"k\": k,\n",
    "            \"rouge1_avg\": rouge1_avg,\n",
    "            \"rouge2_avg\": rouge2_avg,\n",
    "            \"rougeL_avg\": rougeL_avg,\n",
    "            \"rougeLsum_avg\": rougeLsum_avg,\n",
    "            \"meteor_avg\": meteor_avg,\n",
    "            \"bleu_avg\": bleu_avg,\n",
    "            \"precision1_avg\": precision1_avg,\n",
    "            \"precision2_avg\": precision2_avg,\n",
    "            \"precision3_avg\": precision3_avg,\n",
    "            \"precision4_avg\": precision4_avg,\n",
    "            \"brevity_penalty_avg\": brevity_penalty_avg,\n",
    "            \"length_ratio_avg\": length_ratio_avg,\n",
    "            \"translation_length_avg\": translation_length_avg,\n",
    "            \"reference_length_avg\": reference_length_avg\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(resp):\n",
    "    return resp.split(\"\\n---------------------\\n\")[1].split(\"\\n\\n\")\n",
    "\n",
    "def sw_query(index, df, top_k, window, overlap, min_k=1):\n",
    "    for k in range(min_k, top_k+1):\n",
    "        query_engine = index.as_query_engine(similarity_top_k=k)\n",
    "\n",
    "        resps = []\n",
    "        for q in tqdm(df[\"question\"].values, desc=f\"({window}-{overlap} token, k = {k}) Get responses\"):\n",
    "            resps.append(get_response(query_engine.query(q).response))\n",
    "\n",
    "        df[f\"answer_sw_{window}_{overlap}_k{k}\"] = resps\n",
    "    \n",
    "    answer_df = df.loc[:, df.columns.str.contains('answer', case=False)]\n",
    "\n",
    "    return answer_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"../data/\"\n",
    "\n",
    "Settings.llm = None\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size_list = [512, 128, 64, 32, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 token based cross validation medquad variants\n",
    "\n",
    "# medquad = medquad[:1000] #v1\n",
    "# medquad = medquad[2000:3000] #v2\n",
    "# medquad = medquad[4000:5000] #v3\n",
    "# medquad = medquad[6000:7000] #v4\n",
    "# medquad = medquad[8000:9000] #v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medquad = pd.read_json(f\"{dir_data}validations/mqdquad.json\", orient=\"records\")\n",
    "medquad = medquad[:1000]\n",
    "\n",
    "for size in token_size_list:  \n",
    "    storage_context_tb = StorageContext.from_defaults(persist_dir=f\"{dir_data}vectors/token_based_{size}\")\n",
    "    index_tb = load_index_from_storage(storage_context_tb)\n",
    "\n",
    "    answer_df = tb_query(index_tb, medquad, 10, size)\n",
    "    answer_df.to_csv(f\"{dir_data}new validation datas/1000_tb_{size}_answers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in token_size_list:\n",
    "    df_tb = pd.read_csv(f\"{dir_data}new validation datas/1000_tb_{size}_answers.csv\")\n",
    "\n",
    "    scores_df = compute_scores_tb(df_tb, size)\n",
    "\n",
    "    scores_df.to_csv(f\"{dir_data}new validation datas/1000_tb_{size}_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in token_size_list:\n",
    "    scores_df = pd.read_csv(f\"{dir_data}new validation datas/1000_tb_{size}_scores.csv\")\n",
    "\n",
    "    mean_df = compute_mean_scores_tb(scores_df, size)\n",
    "    mean_df.to_csv(f\"{dir_data}new validation datas/1000_tb_{size}_scores_avg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medquad = pd.read_json(f\"{dir_data}validations/mqdquad.json\", orient=\"records\")\n",
    "medquad = medquad[:1000]\n",
    "\n",
    "storage_context_sb = StorageContext.from_defaults(persist_dir=f\"{dir_data}vectors/sentence_based\")\n",
    "index_sb = load_index_from_storage(storage_context_sb)\n",
    "\n",
    "answer_df = sb_query(index_sb, medquad, 10)\n",
    "answer_df.to_csv(f\"{dir_data}new validation datas/1000_sb_answers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df = pd.read_csv(f\"{dir_data}new validation datas/1000_sb_answers.csv\")\n",
    "\n",
    "scores_df = compute_scores_sb(answer_df)\n",
    "\n",
    "scores_df.to_csv(f\"{dir_data}new validation datas/1000_sb_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(f\"{dir_data}new validation datas/1000_sb_scores.csv\")\n",
    "\n",
    "mean_df = compute_mean_scores_sb(scores_df)\n",
    "\n",
    "mean_df.to_csv(f\"{dir_data}new validation datas/1000_sb_scores_avg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 128\n",
    "overlaps = [64, 32, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medquad = pd.read_json(f\"{dir_data}validations/mqdquad.json\", orient=\"records\")\n",
    "medquad = medquad[:1000]\n",
    "\n",
    "for o in overlaps:\n",
    "    storage_context_sw = StorageContext.from_defaults(persist_dir=f\"{dir_data}vectors/token_based_sliding_window_{window}_{o}\")\n",
    "    index_sw = load_index_from_storage(storage_context_sw)\n",
    "\n",
    "    answer_df = sw_query(index_sw, medquad, 10, window, o)\n",
    "\n",
    "    answer_df.to_csv(f\"{dir_data}new validation datas/1000_sw_{window}_{o}_answers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in overlaps:\n",
    "    sw_df = pd.read_csv(f\"{dir_data}new validation datas/1000_sw_{window}_{o}_answers.csv\")\n",
    "\n",
    "    scores_df = compute_scores_sw(sw_df, window, o)\n",
    "\n",
    "    scores_df.to_csv(f\"{dir_data}new validation datas/1000_sw_{window}_{o}_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in overlaps:\n",
    "    scores_df = pd.read_csv(f\"{dir_data}new validation datas/1000_sw_{window}_{o}_scores.csv\")\n",
    "\n",
    "    mean_df = compute_mean_scores_sw(scores_df, window, o)\n",
    "\n",
    "    mean_df.to_csv(f\"{dir_data}new validation datas/1000_sw_{window}_{o}_scores_avg.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
