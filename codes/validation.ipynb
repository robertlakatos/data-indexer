{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext \n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LLM to None\n",
    "Settings.llm = None\n",
    "\n",
    "# Set Hugging Face embedding model for LlamaIndex\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mrr_from_chunks(search_results, full_answer):\n",
    "#     \"\"\"\n",
    "#     MRR kiszámítása a keresőmotor által visszaadott szöveges chunkokból.\n",
    "    \n",
    "#     :param search_results: A szemantikus kereső által visszaadott találatok listája (chunkok).\n",
    "#     :param full_answer: A teljes, helyes válasz (string).\n",
    "#     :return: MRR érték.\n",
    "#     \"\"\"\n",
    "#     reciprocal_ranks = []\n",
    "    \n",
    "#     found_relevant = False\n",
    "#     for rank, chunk in enumerate(search_results, start=1):\n",
    "#         # Ellenőrizzük, hogy a chunk tartalmazza-e a teljes választ (vagy annak egy részét).\n",
    "#         if chunk in full_answer:\n",
    "#             reciprocal_ranks.append(1 / rank)\n",
    "#             found_relevant = True\n",
    "#             break\n",
    "    \n",
    "#     # Ha nincs releváns találat, adj hozzá 0-t\n",
    "#     if not found_relevant:\n",
    "#         reciprocal_ranks.append(0)\n",
    "\n",
    "#     # MRR kiszámítása\n",
    "#     mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "#     return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_found(founds, answer):\n",
    "    results = [0] * len(answer)\n",
    "    real = [1] * len(answer)\n",
    "\n",
    "    for i in range(0, len(answer)):\n",
    "        if answer[i] in founds:\n",
    "            results[i] = 1\n",
    "\n",
    "    acc = accuracy_score(real, results)\n",
    "    f1 = f1_score(real, results, average='weighted')\n",
    "    recall = recall_score(real, results)\n",
    "    precision = precision_score(real, results, average='weighted')\n",
    "\n",
    "    return acc, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_found_af(founds, answer):\n",
    "#     results = [0] * len(founds)\n",
    "#     real = [1] * len(founds)\n",
    "\n",
    "#     for i in range(0, len(founds)):\n",
    "#         if founds[i] in answer:\n",
    "#             results[i] = 1\n",
    "\n",
    "#     acc = accuracy_score(real, results)\n",
    "#     f1 = f1_score(real, results, average='weighted')\n",
    "#     recall = recall_score(real, results)\n",
    "#     precision = precision_score(real, results, average='weighted')\n",
    "\n",
    "#     return acc, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(resp):\n",
    "    return resp.split(\"\\n---------------------\\n\")[1].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medquad = pd.read_json(f\"{dir_data}validations/mqdquad.json\", orient=\"records\")\n",
    "medquad.info()\n",
    "medquad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1640 entries, 0 to 1639\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  1640 non-null   object\n",
      " 1   answer    1640 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 25.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the treatments for dihydropyrimidinas...</td>\n",
       "      <td>These resources address the diagnosis or manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is at risk for Parasites - Cysticercosis? ?</td>\n",
       "      <td>Cysticercosis is an infection caused by the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is (are) phenylketonuria ?</td>\n",
       "      <td>Phenylketonuria (commonly known as PKU) is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Laron syndrome inherited ?</td>\n",
       "      <td>Is Laron syndrome inherited? Most cases of Lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the treatments for globozoospermia ?</td>\n",
       "      <td>These resources address the diagnosis or manag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the treatments for dihydropyrimidinas...   \n",
       "1    Who is at risk for Parasites - Cysticercosis? ?   \n",
       "2                    What is (are) phenylketonuria ?   \n",
       "3                      Is Laron syndrome inherited ?   \n",
       "4      What are the treatments for globozoospermia ?   \n",
       "\n",
       "                                              answer  \n",
       "0  These resources address the diagnosis or manag...  \n",
       "1  Cysticercosis is an infection caused by the la...  \n",
       "2  Phenylketonuria (commonly known as PKU) is an ...  \n",
       "3  Is Laron syndrome inherited? Most cases of Lar...  \n",
       "4  These resources address the diagnosis or manag...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_mq = int(len(medquad)*0.1)\n",
    "medquad_sp = medquad.sample(n=len_mq, random_state=42).copy()\n",
    "medquad_sp = medquad_sp.reset_index(drop=True)\n",
    "medquad_sp.info()\n",
    "medquad_sp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context_sb = StorageContext.from_defaults(persist_dir=\"../data/vectors/sentence_based\")\n",
    "index_sb = load_index_from_storage(storage_context_sb)\n",
    "query_engine_sb = index_sb.as_query_engine(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps = get_response(query_engine_sb.query(medquad[\"question\"].values[0]).response)\n",
    "map_found(resps, sent_tokenize(medquad[\"answer\"].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_metric = evaluate.load(\"f1\")\n",
    "# results = f1_metric.compute(predictions=[\"\\n\". join(resps)], references=[medquad[\"answer\"].values[0]])\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resps,\"\\n\")\n",
    "\n",
    "sent_tokenize(medquad[\"answer\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps = []\n",
    "for q in tqdm(medquad[\"question\"].values):\n",
    "    resps.append(get_response(query_engine_sb.query(q).response))\n",
    "\n",
    "medquad[\"answer_sb_k1\"] = resps\n",
    "\n",
    "medquad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [0] * len(medquad)\n",
    "recalls = [0] * len(medquad)\n",
    "\n",
    "for i in tqdm(range(0,len(medquad))):\n",
    "    acc, f1, recall, prec = map_found(\n",
    "        medquad[\"answer_sb_k1\"].values[i], \n",
    "        sent_tokenize(medquad[\"answer\"].values[i]))\n",
    "    \n",
    "    #print(acc,f1,recall,prec)\n",
    "    accs[i] = acc\n",
    "    # f1s[i] = f1\n",
    "    recalls[i] = recall\n",
    "    # precisions[i] = prec\n",
    "\n",
    "medquad[\"ACC_sb_k1\"] = accs\n",
    "medquad[\"RECALL_sb_k1\"] = recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = medquad[\"question\"].values[10]\n",
    "real_answer  = medquad[\"answer\"].values[10]\n",
    "fake_answer = medquad[\"answer\"].values[5] + medquad[\"answer\"].values[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_answer_sentence = fake_answer.split(\".\")\n",
    "fake_answer_sentence = [item.strip() for item in fake_answer_sentence if len(item) > 2]\n",
    "random.seed(10)\n",
    "random.shuffle(fake_answer_sentence)\n",
    "fake_answer_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_answer_sentence = real_answer.split(\".\")\n",
    "real_answer_sentence = [item.strip() for item in real_answer_sentence if len(item) > 2]\n",
    "random.seed(10)\n",
    "random.shuffle(real_answer_sentence)\n",
    "real_answer_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_found = (real_answer_sentence[:5] + fake_answer_sentence[:3])\n",
    "random.seed(22)\n",
    "random.shuffle(demo_found)\n",
    "demo_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1, recall, precision = map_found(demo_found, real_answer)\n",
    "\n",
    "print(\"Accuracy scores: \", acc)\n",
    "print(\"f1 scores\", f1)\n",
    "print(\"Recall scores\", recall)\n",
    "print(\"Precision scores\", precision)\n",
    "print(\"Mean Reciprocal Rank (MRR): \", calculate_mrr_from_chunks(demo_found, real_answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
