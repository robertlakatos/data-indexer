{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexer pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\opell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Settings, VectorStoreIndex, Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_original = \"../data/\"\n",
    "\n",
    "dir_data_json = f\"../data/validations/mqdquad.json\"\n",
    "dir_data_csv = f\"../data/validations/mqdquad.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_size = 50\n",
    "\n",
    "fixedlength_chunk_size = 512\n",
    "fixedlength_chunk_max_length = 1024\n",
    "\n",
    "# token-based sliding window\n",
    "# 128 - 64 - 32 - 16\n",
    "# 50% overlap okés, de lehetne 25% is\n",
    "# 64 - 16, 32 - 8, 16 - 4, 8 - 2\n",
    "\n",
    "token_sliding_window_size = 128\n",
    "token_sliding_window_overlap = 64\n",
    "\n",
    "sentence_sliding_window_size = 5\n",
    "sentence_sliding_window_overlap = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Dataset (MedQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   answer  50 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 532.0+ bytes\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29d6165cc38498395d98d0a8e712e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7084a862fa2c4c37b152bb7efebc27be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175bbcd7f8c54c27a0fb60d70a689807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed1aacbf20340a9a7eed08d3932a55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e0e96314104519a7ba69080b8148eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91da4b3f0a5a4cf6ab10d54fd1717ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2388a5e09e468693eef0d6af4dec95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d176baf0e664312bae049f31b14d8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e659d6e7634a3082c464ba6d73e829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a095febee64869be9f7bb95e8b8eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240ed8889391407cb91a645697e351b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1262f0b620443cacb3484efd995049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbe8c3d05174ac5bf6c066d106ccdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a05d654cdf6409896724b8dcd4794fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397464958f8242c198fed00f3ef85f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "medquad = pd.read_json(dir_data_json, orient=\"records\")\n",
    "\n",
    "medquad['answer'].to_csv(dir_data_csv, index=False)\n",
    "\n",
    "medquadcsv = pd.read_csv(dir_data_csv)\n",
    "\n",
    "medquadcsv = medquadcsv[:test_file_size]\n",
    "\n",
    "medquadcsv.info()\n",
    "#medquadcsv.head(10)\n",
    "\n",
    "# Set LLM to None\n",
    "Settings.llm = None\n",
    "\n",
    "# Set Hugging Face embedding model for LlamaIndex\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "# Hugging Face tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateQueries(query_engine):\n",
    "    # Define your queries\n",
    "    queries = [\n",
    "        \"What is (are) keratoderma with woolly hair?\",\n",
    "        \"How many people are affected by keratoderma with woolly hair?\",\n",
    "        \"What are the genetic changes related to keratoderma with woolly hair?\",\n",
    "        \"Is keratoderma with woolly hair inherited?\",\n",
    "        \"What are the treatments for keratoderma with woolly hair?\",\n",
    "        \"What is (are) Knobloch syndrome?\",\n",
    "        \"How many people are affected by Knobloch syndrome?\",\n",
    "        \"What are the genetic changes related to Knobloch syndrome?\"\n",
    "    ]\n",
    "\n",
    "    # Iterate through the queries and print responses\n",
    "    for query in queries:\n",
    "        response = query_engine.query(query)\n",
    "        print(f\"Query: {query}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rögzített hosszúságú chunk-ok (Fixed-length chunking)\n",
    "\n",
    "- **Hivatkozás:** Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 4171-4186). https://doi.org/10.18653/v1/N19-1423\n",
    "- **Állítás:** A rögzített hosszúságú chunk-ok használata gyakori a BERT alapú modellekben, ahol a dokumentumokat egy meghatározott hosszúságú (512 tokenes) egységekbe bontják."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create fixed-length chunks using tokenizer\n",
    "def create_fixed_length_chunks_with_tokenizer(texts, chunk_size=fixedlength_chunk_size):\n",
    "    chunks = []\n",
    "    for text in tqdm(texts, desc=\"Creating chunks\"):\n",
    "        tokens = tokenizer.tokenize(text, max_length=fixedlength_chunk_max_length, truncation=True)  # Tokenize the text using Hugging Face tokenizer\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            chunk_tokens = tokens[i:i + chunk_size]\n",
    "            chunk = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "            chunks.append(Document(text=chunk)) # Store the chunk as a Document\n",
    "    return chunks\n",
    "\n",
    "# Extract the 'answer' column from the loaded data\n",
    "texts = medquadcsv['answer'].tolist()\n",
    "\n",
    "# Create chunks from the 'answer' column text\n",
    "document_chunks_fixed_length = create_fixed_length_chunks_with_tokenizer(texts)\n",
    "\n",
    "# Create a VectorStoreIndex from the document chunks\n",
    "index_fixed_length = VectorStoreIndex.from_documents(document_chunks_fixed_length, show_progress=True)\n",
    "\n",
    "index_fixed_length.storage_context.persist()\n",
    "\n",
    "# Set up the query engine\n",
    "query_engine_fixed_length = index_fixed_length.as_query_engine()\n",
    "\n",
    "CreateQueries(query_engine_fixed_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window (csúszóablak) módszer, token hossz átfedéssel\n",
    "\n",
    "- **Hivatkozás:** Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. https://arxiv.org/abs/2004.05150\n",
    "- **Állítás:** Az idézet alátámasztja a csúszóablakos módszert, amely csökkenti a hosszú szövegek feldolgozásának költségeit, miközben megőrzi a szöveg összefüggését, és a dokumentum több ezer tokenes feldolgozását is lehetővé teszi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sliding window chunks based on tokens\n",
    "def create_token_based_chunks_with_sliding_window(texts, chunk_size=token_sliding_window_size, overlap=token_sliding_window_overlap):\n",
    "    chunks = []\n",
    "    stride = chunk_size - overlap  # Calculate the stride based on chunk size and overlap\n",
    "    for text in tqdm(texts, desc=\"Creating chunks\"):\n",
    "        tokens = tokenizer.tokenize(text, max_length=fixedlength_chunk_max_length, truncation=True)  # Tokenize the text using Hugging Face tokenizer\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i:i + chunk_size]  # Select tokens within the window size\n",
    "            chunk = tokenizer.convert_tokens_to_string(chunk_tokens)  # Convert tokens back to string\n",
    "            chunks.append(Document(text=chunk))  # Store the chunk as a Document\n",
    "    return chunks\n",
    "\n",
    "# Extract the 'answer' column from the loaded data\n",
    "texts = medquadcsv['answer'].tolist()\n",
    "\n",
    "# Create token-based chunks using sliding window\n",
    "document_chunks_sliding_window_chunk_based = create_token_based_chunks_with_sliding_window(texts)\n",
    "\n",
    "# Create a VectorStoreIndex from the document chunks\n",
    "index_sliding_window_chunk_based = VectorStoreIndex.from_documents(document_chunks_sliding_window_chunk_based, show_progress=True)\n",
    "\n",
    "index_sliding_window_chunk_based.storage_context.persist()\n",
    "\n",
    "# Set up the query engine\n",
    "query_engine_sliding_window_chunk_based = index_sliding_window_chunk_based.as_query_engine()\n",
    "\n",
    "CreateQueries(query_engine_sliding_window_chunk_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Mondat- vagy bekezdés-alapú darabolás (Sentence/Paragraph-based chunking)\n",
    "\n",
    "- **Hivatkozás:** Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., ... & Petrov, S. (2019). Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7, 453-466. https://doi.org/10.1162/tacl_a_00276\n",
    "- **Állítás:** Itt a mondat- és bekezdés-alapú chunking módszer a kérdés-válasz rendszerek fejlesztésében alkalmazott módszert illusztrálja, amely függetlenül kezeli a hosszú és rövid válaszokat a dokumentumból kinyert mondat- vagy bekezdésszintű egységek alapján."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sentence chunks: 100%|██████████| 50/50 [00:00<00:00, 3223.56it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f2e2288eeb4d9886c6890217384dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cea924f5a0c41ba8f20b2b8d828f205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to create sentence-based chunks\n",
    "def create_sentence_based_chunks(texts):\n",
    "    chunks = []\n",
    "    for text in tqdm(texts, desc=\"Creating sentence chunks\"):\n",
    "        # Simple sentence splitting based on common delimiters\n",
    "        sentences = sent_tokenize(text)\n",
    "        #sentences = text.replace('!', '.').replace('?', '.').split('.')  # Replace punctuation and split\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()  # Remove leading/trailing whitespace\n",
    "            if sentence:  # Ensure the sentence is not empty\n",
    "                chunks.append(Document(text=sentence))  # Store each sentence as a Document\n",
    "    return chunks\n",
    "\n",
    "# Extract the 'answer' column from the loaded data\n",
    "texts = medquadcsv['answer'].tolist()\n",
    "\n",
    "# Create chunks from the 'answer' column text\n",
    "document_chunks_sentence_based = create_sentence_based_chunks(texts)\n",
    "\n",
    "# Create a VectorStoreIndex from the document chunks\n",
    "index_sentence_based = VectorStoreIndex.from_documents(document_chunks_sentence_based, show_progress=True)\n",
    "\n",
    "index_sentence_based.storage_context.persist(\"../data/vectors/sentence_based\")\n",
    "\n",
    "# Set up the query engine\n",
    "query_engine_sentence_based = index_sentence_based.as_query_engine()\n",
    "\n",
    "# CreateQueries(query_engine_sentence_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window (csúszóablak) módszer, mondatonként\n",
    "\n",
    "- **Hivatkozás:** Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. https://arxiv.org/abs/2004.05150\n",
    "- **Állítás:** Az idézet alátámasztja a csúszóablakos módszert, amely csökkenti a hosszú szövegek feldolgozásának költségeit, miközben megőrzi a szöveg összefüggését, és a dokumentum több ezer tokenes feldolgozását is lehetővé teszi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sentence-based chunks with sliding window\n",
    "def create_sliding_window_chunks(texts, window_size=sentence_sliding_window_size, stride=sentence_sliding_window_overlap):\n",
    "    chunks = []\n",
    "    for text in tqdm(texts, desc=\"Creating sentence chunks\"):\n",
    "        # Simple sentence splitting based on common delimiters\n",
    "        # sentences = text.replace('!', '.').replace('?', '.').split('.')  # Replace punctuation and split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence]  # Clean up empty and whitespace sentences\n",
    "        \n",
    "        # Apply sliding window\n",
    "        for i in range(0, len(sentences), stride):\n",
    "            window_sentences = sentences[i:i + window_size]  # Select a window of sentences\n",
    "            if window_sentences:  # If the window has sentences\n",
    "                chunk = ' '.join(window_sentences)  # Join the sentences to form a chunk\n",
    "                chunks.append(Document(text=chunk))  # Store each chunk as a Document\n",
    "    return chunks\n",
    "\n",
    "# Extract the 'answer' column from the loaded data\n",
    "texts = medquadcsv['answer'].tolist()\n",
    "\n",
    "# Create sliding window chunks from the 'answer' column text\n",
    "document_chunks_sliding_window_sentence_based = create_sliding_window_chunks(texts)\n",
    "\n",
    "# Create a VectorStoreIndex from the document chunks\n",
    "index_sliding_window_sentence_based = VectorStoreIndex.from_documents(document_chunks_sliding_window_sentence_based, show_progress=True)\n",
    "\n",
    "index_sliding_window_sentence_based.storage_context.persist()\n",
    "\n",
    "# Set up the query engine\n",
    "query_engine_sliding_window_sentence_based = index_sliding_window_sentence_based.as_query_engine()\n",
    "\n",
    "CreateQueries(query_engine_sliding_window_sentence_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchikus chunking\n",
    "\n",
    "- **Hivatkozás:** Luong, T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 1412-1421). https://doi.org/10.18653/v1/D15-1166\n",
    "- **Állítás:** A hierarchikus chunking megközelítés itt úgy jelenik meg, hogy a figyelmet több szinten alkalmazzák (szó- és mondatszinten), ami lehetővé teszi a modellek számára, hogy a különböző szintű kontextusokat figyelembe vegyék a szövegek feldolgozása során."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_chunks_2_levels(texts):\n",
    "    top_level_chunks = create_fixed_length_chunks_with_tokenizer(texts)  # First-level chunking based on tokens\n",
    "    hierarchical_chunks = []\n",
    "    \n",
    "    for chunk in top_level_chunks:\n",
    "        chunk_text = chunk.text  # Text of the first-level chunks\n",
    "        sentence_based_chunks = create_sentence_based_chunks([chunk_text])  # Second-level chunking based on sentences\n",
    "        hierarchical_chunks.extend(sentence_based_chunks)  # Collect sentence-based chunks\n",
    "    \n",
    "    return hierarchical_chunks\n",
    "\n",
    "# Extract the 'answer' column from the loaded data\n",
    "texts = medquadcsv['answer'].tolist()\n",
    "\n",
    "# Create hierarchical chunks from the 'answer' column text\n",
    "document_chunks_hierarchical = create_hierarchical_chunks_2_levels(texts)\n",
    "\n",
    "# Create a VectorStoreIndex from the hierarchical chunks\n",
    "index_hierarchical_2_levels = VectorStoreIndex.from_documents(document_chunks_hierarchical, show_progress=True)\n",
    "\n",
    "index_hierarchical_2_levels.storage_context.persist()\n",
    "\n",
    "# Set up the query engine\n",
    "query_engine_hierarchical_2_levels = index_hierarchical_2_levels.as_query_engine()\n",
    "\n",
    "CreateQueries(query_engine_hierarchical_2_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_chunks_3_levels(texts):\n",
    "    top_level_chunks = create_fixed_length_chunks_with_tokenizer(texts )# First level: Chunking based on tokens (fixed length chunks)\n",
    "    hierarchical_chunks = []\n",
    "    \n",
    "    # Second level: Further chunking each token-based chunk into sentences\n",
    "    for chunk in top_level_chunks:\n",
    "        chunk_text = chunk.text  # Text of the first-level chunks\n",
    "        sentence_based_chunks = create_sentence_based_chunks([chunk_text])  # Second-level chunking based on sentences\n",
    "        \n",
    "        # Third level: Apply sliding window on sentence chunks\n",
    "        for sentence_chunk in sentence_based_chunks:\n",
    "            sentence_chunk_text = sentence_chunk.text  # Text of the second-level (sentence-based) chunks\n",
    "            sliding_window_chunks = create_sliding_window_chunks([sentence_chunk_text])  # Third-level chunking with sliding window\n",
    "            hierarchical_chunks.extend(sliding_window_chunks)  # Collect all sliding window-based chunks\n",
    "\n",
    "    return hierarchical_chunks\n",
    "\n",
    "# Extract the 'answer' column from the loaded data\n",
    "texts = medquadcsv['answer'].tolist()\n",
    "\n",
    "# Create three-level hierarchical chunks from the 'answer' column text\n",
    "document_chunks_hierarchical_3_levels = create_hierarchical_chunks_3_levels(texts)\n",
    "\n",
    "# Create a VectorStoreIndex from the three-level hierarchical chunks\n",
    "index_hierarchical_3_levels = VectorStoreIndex.from_documents(document_chunks_hierarchical_3_levels, show_progress=True)\n",
    "\n",
    "index_hierarchical_3_levels.storage_context.persist()\n",
    "\n",
    "# Set up the query engine\n",
    "query_engine_hierarchical_3_levels = index_hierarchical_3_levels.as_query_engine()\n",
    "\n",
    "CreateQueries(query_engine_hierarchical_3_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dinamikus chunking tokenlimittel\n",
    "\n",
    "- **Hivatkozás:** Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. Advances in Neural Information Processing Systems, 33, 1877-1901. https://arxiv.org/abs/2005.14165\n",
    "- **Állítás:** A dinamikus chunking módszer ebben a kontextusban figyelmet fordít arra, hogy a természetes mondathatárok mentén darabolja fel a szöveget, így a GPT-3 modell optimálisan használja a 2048 tokenes kontextusablakát anélkül, hogy fontos információkat truncálna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addig teszi a mondatokat ameddig a vektora eléggé hasonlit az előzőig, mennyire hasonlit? 0.75\n",
    "# engedjük el :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
