{
  "meta": {
    "title": "A review of machine learning techniques for identifying weeds in corn",
    "author": "Akhil Venkataraju, Dharanidharan Arumugam, Calvin Stepan, Ravi Kiran, Thomas Peters",
    "journal": "Elsevier BV",
    "date": "2023. 02."
  },
  "paragraphs": [
    {
      "title": "Abstract",
      "div": [
        {
          "text": "Weeds pose a major challenge in achieving high yield production in corn. The use of herbicides although effective can be expensive and their excessive use poses ecological concerns and herbicide resistance. Precise identification of weeds using Machine Learning (ML) models significantly reduces the use of herbicides. In this study, we provide a brief overview of the important ML methods used for identifying weeds in corn i.e., classification and object detection. The various metrics that are used for the evaluation of the performance of ML methods are also discussed. In the end, we identify some important research gaps which warrant future investigation. Most ML methods for the identification of weeds use digital images as input data, however, in some cases, hyperspectral data were used. Most of the current studies employ support vector machines and neural networks for the identification of weeds. Classification accuracy and F1 score are the two most frequently used accuracy metrics to evaluate the performance of ML models used. Future research on the identification of weeds may focus on improving the data volume using data augmentation, transfer learning to benefit from existing models, and interpretability of neural networks to avoid overfitting and make models more transparent.",
          "table": []
        }
      ]
    },
    {
      "title": "Introduction",
      "div": [
        {
          "text": "Corn (Zea mays) is one of the most consumed commodities in the world including the United States. The United States alone accounted for 26% of the world's corn consumption during 2020-2021 . The United States is the world's largest corn producer, also accounting for 345 million metric tons in 2019-2020 . Corn is considered a highly productive crop because of its wide array of industrial and agricultural uses such as animal feeds, biofuels, and food sweeteners. According to the 2020 US corn usage issued by USDA (United States of Agricultural Department), a major proportion of the produced corn (roughly 46%) was used as animal feed and roughly 27% was used for biofuel production. A significant portion of the rest (roughly 18%) was exported majorly to countries such as Colombia, South Korea, Japan, and Mexico. South Korea and Japan rely on corn from the United States for their animal feed. The rest of the production (roughly 9%) was used in making products such as corn syrups, sweeteners, corn starches, cereals, and beverages. Corn-based deicing materials , corrosion inhibitors , and coating materials  are recently being developed and these products can further expand corn usage in the domestic market. Corn being a highly productive crop plays a major role in the US economy. According to a study conducted by Corn Refiners Association , the corn refining industry alone featured 47.5 billion dollars in economic output in the United States in 2020.",
          "table": []
        },
        {
          "text": "One of the major challenges in achieving higher corn productivity is the control and management of weed growth. Weeds, in addition to competing with the corn for the nutrients and crop resources, also introduce harmful bacteria, viruses, and other microorganisms which results in significant yield loss. According to a study  conducted based on the USDA-NASS (United States Department of Agriculture -National Agricultural Statistics Service) 2014 corn yield report, the interference of weeds in corn production resulted in an average 50% yield loss annually from 2007 to 2013. In this period, weed interference resulted in an average loss of $26.7 billion annually in terms of economic value. Weed is commonly controlled through the application of herbicides or removal through mechanical, thermal, and electrical means. Out of these two broad approaches, the application of herbicides is the prevalently used one. However, the use of herbicides involves several drawbacks. Applying herbicides to the entire field is very expensive. Herbicides cost roughly $60 per acre which is 10% of the expected market revenue of the corn as per the 2021 Purdue Crop Cost and Return Guide . Excessive use of herbicides is also detrimental to soil fertility, the aquatic ecosystem, and human health. Furthermore, weeds develop resistance to herbicides over time. Selective spraying of herbicides would address these shortcomings and also cut down the cost of the herbicides. Selective spraying of herbicides or the removal of weeds requires precise identification of weeds. Hence, the identification of weeds plays an important role in the management and control of weeds. Given the scale of the problem, manual identification of weeds is either untenable or impractical in many situations. ML techniques are successfully applied for the precise identification of weeds. The use of machine learning techniques also made the automation of weed control and management possible.",
          "table": []
        },
        {
          "text": "This review surveyed the various ML approaches that were applied over the years for the identification of weeds in cornfields. We also describe in full technical detail, the type of ML problem solved (classification, object detection etc.), the type of weeds identified, the type of data used, the type of error metrics used to evaluate the performances of these approaches. These ML approaches are grouped into three major categories namely, SVM, Neural Networks, and Miscellaneous. Section 3 describes the first category, SVM, Section 4 discusses Neural Network approaches, and Section 5 elaborates on the miscellaneous ML techniques used in the past for the identification of weeds in cornfields. Section 6 explains the importance of data for the performance of the ML techniques and the various metrics that are used to evaluate the performance of these techniques. Section 7 briefly discusses the conclusion and future research directions of ML-based identification of weeds. List of abbreviations. artificial neural network summarizes the abbreviations that are used in this study.",
          "table": [
            "List of abbreviations. artificial neural network"
          ]
        }
      ]
    },
    {
      "title": "Machine learning",
      "div": [
        {
          "text": "Machine Learning is a class of Artificial Intelligence (AI) that focuses on aiding the computers in learn the underlying relationship between inputs and outputs from the given data and make accurate predictions . ML algorithms employ statistical methods to learn from the exposed data without any explicit programming instructions . The workflow of a typical ML model is as depicted in Fig. 1 and consists of the following phases: Data acquisitiongathering data (open-source datasets, sensors, etc.) Data pre-processinginvolves cleaning the data, making the data suitable to be used by the model Dataset creationinvolves splitting the data into training, validation, and testing sets Model trainingthe training set is used to train the model and the model learns appropriate input-output relationships Model testing and performance evaluation-the trained model is employed on the testing set and performance metrics are used to quantify the model's accuracy Model deploymentmaking the model available to the users via web/software application During the process of model training, the model's performance may degrade due to seasonal/abrupt changes in data patterns that may occur over time, hence, the model must be updated, and the workflow cycle must be completed by returning to the data acquisition phase. Also, the parameters whose values are set before the start of a learning process i. e., hyperparameters need to be tuned to better the positive results achieved during the process of model evaluation. This is mainly done to control the overall behavior of the model. Model deployment has become an important element in the modern practice as it focuses on the production standpoint of the ML models. MLOps offer comprehensive steps and methodology to address the deployment aspect of the ML models. More details about MLOps can be found here . ML is split into the following categories depending on the sort of learning response accessible to a learning system: Supervised learning: Supervised learning includes training the ML algorithms with labeled data to perform tasks like classification or regression. Labeled datasets contain both the causal factors (input features) and their target responses or outcomes (output variables). The causal factors are the input features, and the target responses are the output variables. The objective of supervised learning is to identify the underlying relationship between input features and output variables so that they can predict the target responses for future unforeseen input features. Some of the applications of supervised learning in precision agriculture can be found in  Unsupervised learning: Unsupervised ML algorithms are trained on an unlabeled dataset to identify patterns and structures in the given data. They are mainly used for tasks such as clustering and features association. K-means clustering, Principal Component Analysis (PCA) and Gaussian Mixture Models are popular unsupervised learning algorithms. Davis et al.  show some of the practical applications of unsupervised learning in precision agriculture.",
          "table": []
        },
        {
          "text": "Reinforcement learning: Reinforcement learning is a paradigm of machine learning which involves a sequential decision-making process to achieve the end goal. In reinforcement learning, a computer agent learns to reach a defined goal optimally by navigating through the environment by choosing actions that yield higher rewards. SARSA, and Deep Q networks are some of the widely used algorithms for reinforcement learning. The application of reinforcement learning in precision agriculture can be found here  In the following three sections, we provide a brief introduction of popular ML methods and extensively analyze the use of these methods for identification of weeds in corn.",
          "table": []
        }
      ]
    },
    {
      "title": "Support Vector Machines",
      "div": [
        {
          "text": "Support Vector Machines (SVMs) is a non-heuristic classification method that was initially introduced as a linear classifier by Vapnik and Chervonenkis in 1963 . SVMs construct hyperplanes to classify the given data set. The most defining aspect of it is it tends to identify a hyperplane that maximizes the margin between the support vectors. The support vectors are planes that pass through the nearest data points of the individual classes. This maximization of margin leads to a reduction in the generalization error. SVMs can use two types of margins: hard margin and soft margin. Hard margin is employed when the given data is linearly separable and noiseless. However, the use of a hard margin often leads to the overfitting of the data so, the soft margin is used to improve the generalization of the noisy data set. In the soft margin approach, overlapping data points are weighed down to allow slack in the classification. Cortes and Vapnik  introduced the soft margin approach in 1995 and successfully applied it to recognize handwritten images. In Boser et al.  further developed SVMs to solve non-linear classification problems using an approach called 'Kernel Trick'. Kernel trick maps the inputs into a higher dimensional space so that it becomes separable by linear hyperplanes. SVMs are also extended to solve regression and multi-class classification problems . SVMs have now become a highly dependable tool for digital image classification, text categorization, character recognition, and many other AI-related tasks. Some of its recent advancements in SVMs can be found in .",
          "table": []
        },
        {
          "text": "SVMs are popularly used for the identification of weeds .",
          "table": []
        },
        {
          "text": "Karimi et al.  employed SVM for the detection of dominant grassy weed and nitrogen stress in corn with the use of hyperspectral data. The data contains 72 narrow bands ranging between 408.73 and 947.07 nm. The major factor considered was weed treatment while the sub-factors were three nitrogen application rates. The classification accuracy of the employed SVM model with RBF kernel yielded an accuracy of 69.2% when the nitrogen rates and weed infestation levels were combined, interestingly when the weed control and nitrogen rates are considered separately the classification accuracy was improved to be more than 80%.",
          "table": []
        },
        {
          "text": "Wu and Wen  investigated SVM as a classifier to identify corn and weed seedlings with the use of texture features. The weeds were Chinese Sprangletop (Leptochloa chinensis (L.) Nees), Yerbadetajo (Ecliptaprostrata L.), Rice galingale (Cyperus iria L.), and Copperleaf (Acalypha australis L.). The dataset comprising 66 color images (30 images of corn and 36 images of weeds, respectively) was transformed to gray-level following which their statistical properties were obtained from their histograms. These, along with GLCM were used to extract the following texture features: smoothness (R), mean (m), entropy (e), third moment (µ 3 ), contrast (F1), energy (F2), homogeneity (F3), and correlation (F4), uniformity (u) and standard deviation (s). The features were replaced by their means i.e., mean of contrast (f1), mean of energy (f2), mean of homogeneity (f3), and mean of correlation (f4). 60% of the data was used to train the SVM while the remaining 40% was used for testing it. SVM with the following features as input vectors yielded the following results: 92.59% testing accuracy for F4 (consisting of f1, f2, f3, and f4) as input vector, 92.31% testing accuracy for F6 (consisting of m, s, R, μ 3 , u and e), 100% testing accuracy for F8 (features selected by PCA, not mentioned) and 100% testing accuracy for F10 (all texture features). When the SVM was input with F8 as the input vector and compared with a BP network, the results showed that the SVM with an accuracy of 100% outperformed the BP network that could only produce an accuracy of 80%.",
          "table": []
        },
        {
          "text": "In a later study, Wu et al. demonstrated the use of shape features as SVM inputs for the classification of corn and weed seedlings . The dataset consisted of 64 RGB images (40 used for training and 24 used for testing). The specific number of images of corn and weed was not mentioned. Citing the images had better features in HIS space than in an RGB space as the reason, the authors transformed the RGB images to HIS space. From these images, the following leaf shape parameters were obtained: R, M, L, and Roundness. These parameters were input to the SVM with three different kernel functions namely, RBF, sigmoid and polynomial. The classification accuracies read: 96.50% for RBF-SVM, 67.67% for sigmoid-SVM, 90.00% for polynomial-SVM and 83.20% for ANN (Artificial Neural Network), respectively.",
          "table": []
        },
        {
          "text": "Ahmed et al.  demonstrated a texture-based weed classification in which LBP was used to obtain textural features. Broadleaf and grassy categories of weeds were studied. For the dataset consisting of 200 color images (100 of each category), ten-fold cross-validation was performed, and the dataset was randomly partitioned into 10 subsets where one of them was used for testing and the remaining nine for training. The LBP operator was used to compute the LBP code for each pixel of the input image thereby resulting in the formation of an encoded representation of the image. With the use of this, a histogram was obtained and used as the feature vector, this feature vector represented the image's texture information. SVM with RBF kernel was used for classification and produced a classification accuracy of 98.5%.",
          "table": []
        },
        {
          "text": "The study by Wong et al.  aimed at classifying weed seedlings using a multi-class SVM that produces the best probabilistic output. GA was used to select the features and fine-tune the classifier parameters, followed by which SVMs were used for classification purposes. The weeds involved in the study were: Phylanthus Urinuria, Agerantum Conyzoides sp., Amaranthus palmeri sp., and other weeds (dicotyledon and monocotyledon). Three SVMs were trained to distinguish monocotyledon weeds from other weeds, Agerantum Conyzoides weeds from other weeds, and Amaranthus Palmeri weeds from other weeds. The following features were selected: single-leaf/overall leaf shape, elliptical Fourier descriptors, regional shape parameters, fractal, Hu moment invariants, skeleton stats, boundary to the centroid, area, and color stats. The data consisting of 400 feature rows were divided into training and verification data in the ratio of 50:50 following which the SVMs were both trained and verified on it. The GA-optimized SVMs were then tested with unseen/external data consisting of 240 datasets in which 100 belonged to Amarathus Palmeri weeds and another 100 belonged to other weeds (no mention of what the remaining 40 belonged to). The population size was equal to 100 and the GA was set in such a way that it would stop when the best fitness does not exceed 40 generations. The dataset used for testing the classification consisted of image samples of weeds belonging to ten species. 450 images were used for testing purposes, and it was observed that all of the categories had a high true positive value of 100%.",
          "table": []
        },
        {
          "text": "In a work that deals with automatic spraying control, conducted by Siddiqui et al. , a system was developed for weed classification using wavelet transform, SWLDA, and SVMs. 46 wavelets belonging to six wavelet families were tested and decomposed up to four levels. The wavelet families included the biorthogonal wavelet family with 10 sub-wavelets, Symlet wavelet family with 10 sub-wavelets, Coiflet wavelet family with 5 sub-wavelets, reverse biorthogonal wavelet family with 10 sub-wavelets, Daubechies wavelet family with 10 sub-wavelets, and discrete Meyer wavelet family. The most meaningful features (not mentioned) were extracted using SWLDA. Finally, these features were fed to the SVMs for classification (into two categories: weeds with narrow leaves and weeds broad leaves, respectively). The authors also included a pre-processing step that eliminated lighting effects thereby ensuring high accuracies in rea1-life scenarios. The data collected consisted of 1200 RGB images (500 each from narrow and broad categories, respectively and 200 from an unknown category) which then underwent decomposition using the wavelets families before SWLDA extracted the most relevant features (not mentioned). The training set consisted of 600 images comprising 250 images of broad leaves, 250 of narrow leaves, and 100 of unknown, and cross-validation was also done. The testing set consisted of the other 600 images comprising of 250 broad leaves, 250 of narrow, and 100 of unknown. The confusion matrix was used as the performance metric and the best result was given by Symlet-SWLDA-SVM (98.1% classification accuracy). The results for the wavelets were: biorthogonal (classification accuracy of 95.66%), Coiflet (classification accuracy of 94.66%), reverse biorthogonal (classification accuracy of 95.33%), Daubechies (classification accuracy of 95.0%) and discrete Meyer (classification accuracy of 93.33%).",
          "table": []
        },
        {
          "text": "Another texture-based classification of weed and crop was demonstrated by Athani and Tejeshwar in their work . The texture features involved in the study were entropy, mean, intensity, smoothness, uniformity, standard deviation, and third moment. Along with texture features, color features and shape features (region descriptors and boundary descriptors) were also considered. SVM was used for classification on a dataset consisting of 1000 color images (500 of maize and 500 of weed (the type/species of weed was not mentioned) and k-fold cross-validation was used too. From this dataset, 450 images of each category were used for training while 50 from each were used for testing. The results showed that the accuracy of prediction for the SVM was 82%.",
          "table": []
        },
        {
          "text": "Another use of shape features for crop and weed classification was demonstrated by Satvini [46]. The following shape features were involved in the study: eccentricity, area, major axis length, perimeter, and minor axis length. Para grass, Chrysanthemum, and Nutsedge were the weeds involved in this study. SVM (with RBF and polynomial functions) was used for classification purposes. 2560 images comprised the dataset out of which, 1155 images of each class (weed and crop) were used for training purposes while 125 images of each class were used to validate the trained model. The performance of the SVM was as follows: correctly classified all 125 images of the crop as crop while it correctly classified 104 out of 125 weed images of weed as weed and misclassified the remaining 21 as a crop. Summary of studies that employed SVMs for the identification of weeds summarizes all the above works that employed SVMs for the identification of weeds.",
          "table": [
            "Summary of studies that employed SVMs for the identification of weeds."
          ]
        }
      ]
    },
    {
      "title": "Neural networks",
      "div": [
        {
          "text": "Convolutional Neural Networks (CNNs) are a class of deep neural networks used for object/ image recognition, classification, and many other computer vision tasks. It mimics the visual cortex of the human brain in recognizing visual patterns and learning the important features and spatial connections in the images with the least amount of preprocessing possible. Hubel and Weisel  1962 discovered that complex cells in the visual cortex achieve spatial invariance by summing the output of different simple receptive cells. Inspired by their work, Fukishma  proposed the first-ever visual recognition model called \"neocognitron\". The proposed neocognitron model consisted of two preprocessing layers called 'S'(Simple) cells and 'C'(complex) cells mirroring the discovery of Hubel and Weisel. Waibel developed a convolutional-like neural network called Time Delay Neural Networks (TDNN) in 1987 to achieve shift-invariance in the temporal dimension . However, the CNN in its present form is first introduced by Lecunn et al.  in 1998. The proposed convolutional neural network called 'LeNet-5 ′ was successfully applied to classify hand-written digital images. The convolutional block of LeNet-5 consisted of feature maps called 'kernels' or 'filters' and pooling layers. Even though LeNet-5 initiated a promising paradigm for computer vision, the non-availability of higher computational units and large image databases dampened its progress. But in 2012, Krizhevsky et al.  were able to successfully scale up the LeNet to a deeper and broader network with a larger image database (Imagenet) and with the use of GPUs. Since then, many advanced neural networks have been developed, such as VGGNet , GoogLeNet , and ResNets  .",
          "table": []
        },
        {
          "text": "CNNs, in general consist of two blocks: 1. Convolutional block and 2. Fully connected neural network block. The convolutional block extracts important features and spatial connections from the images with minimal computation. Images are usually represented in the form of 2-D pixel matrices with multiple channels, for instance, an RGB image has 3 channels of 2-D pixel matrices. These image input matrices are first operated upon by a convolutional block, and the extracted information is flattened into a single column feature vector. This flattened feature vector is then fed into a fully connected neural network and trained using a backpropagation algorithm. In the convolutional block, image matrices are first subjected to convolution, followed by pooling.",
          "table": []
        },
        {
          "text": "Convolution is mainly performed for feature extraction, and it is done using filters or kernels in the form of matrices. Kernel matrices are of a considerably smaller dimension and are chosen appropriately based on the nature of the problem. Convolved features are obtained by taking Hadamard product between the image and the kernel matrices. As the kernel matrices of smaller dimensions compared to image matrices, convolved features are generated by sliding the kernel matrices from left to right and top to bottom and taking the Hadamard product at each position of the kernel matrix on the image matrix. The sliding of the kernel matrix is defined in terms of 'strides'; for example, a stride of allows the kernel filter to shift one column left and one row down. In addition to striding, convolution also involves padding, which adds additional rows and columns of zeros to the input matrices so that the pixel information present in the edges of image matrices is not lost.",
          "table": []
        },
        {
          "text": "The features extracted from convolution are sensitive to the location, and to achieve a translation invariance (less sensitive to the location) of these features, a downsampling operation called 'pooling' is carried out. Like the kernel filter, the pooling filter is also of smaller size compared to the feature maps. The size of the feature maps is usually halved when using pooling filters. For example, the size of 4 × 4 will be converted to 2 × 2. Max pooling and average pooling are the two most common types of pooling filters used in CNN. Average pooling involves the extraction of the average value of map features, whereas max-pooling extracts the maximum value. The choice of pooling depends upon the nature of the given data. Average pooling tends to smoothen the image, whereas maxpooling tends to brighten or select bright pixels from the image. After pooling, a fully connected layer is formed as a single column vector for each example and fed into the neural networks, and, trained using a backpropagation algorithm. For classification problems, CNNs commonly use ReLU, eLU, and tanH for hidden layers and SoftMax activation functions for the output layer.",
          "table": []
        },
        {
          "text": "Moshou et al.  proposed a new neural network architecture, SOM where the neurons are associated with local linear mappings for the classification of crop and weed from their near-infrared reflectance spectra which were obtained with the help of an imaging spectrograph. The dataset consisted of 88 corn samples, 77 samples of the buttercup (Ranunculus repens), 79 samples of Canada thistle (Cirsium arvense), samples of charlock (Sinapis arvensis), 73 samples of chickweed (Stellaria media), 76 samples of dandelion (Tarraxacum officinale), samples of grass (Poa annua), 78 samples of redshank (Poligonum persicaria), 75 samples of stinging nettle (Urtica dioica), 78 samples of wood sorrel (Onalis europaea) and 75 samples of yellow trefoil (Medicago lupulina) resulting in a dataset of 766 and 88 reflectance spectra for weed and corn, respectively. A separability index was used to obtain five principal components and the following wavelengths: 539, 540, 542, 545, 549, 557, 565, 578, 585, 596, 605, 639, 675, 687, 703, 814 and  840. Cross-validation divided the data into 10 equal sets and the neural network was trained and tested with 90% and 10% of the data, respectively for all the 10 sets. The classification accuracy (obtained by averaging the classification rates for all the test sets) was 96% for corn and 90% for weeds. The network also fared better when compared with other classifiers such as PNN (classification accuracy of 85% for corn and 77% for weed), Multi-layer Perceptron (95% and 70% for corn and weed, respectively), SOM (85% and 77% for corn and weed, respectively), and Linear Vector Quantization (85% and 77% for corn and weed, respectively). The work by Yang et al.  demonstrated an approach to classifying weeds in cornfields using ANN. The weeds studied in this work were: common lambsquarters (Chenopodium album L.), yellow nutsedge (Cyperus esculentus L), quackgrass, (Agropyron repens L.), and velvetleaf (Abutilon theophrasti Medik.). The original images (number/size not mentioned) were rotated by 90, 180, and 270 • , respectively thereby resulting in a larger dataset comprising 1736 color images of corn, 772 of velvetleaf, 672 of quackgrass, 752 of common lambsquarters, and 1480 of yellow nutsedge. The greenness method was used to extract green content from the images followed by which the images were converted to grayscale images. Cross-validation was only supposed to be used when it was learned that the ANN was found to not be getting thorough training in the first case. ANN performed the best for corn with a recognition rate of 100%. The results for the weeds were: velvetleaf (92%), quackgrass (62%), and yellow nutsedge (80%).",
          "table": []
        },
        {
          "text": "In the work done by Wu et al. , classification of weeds and corn seedlings were performed using textural features. They also classified weed and corn based on wavelet features and fractal dimensions aiming at the classification of weed and corn . The weeds studied were the monocotyledonous weeds (Goosegrass and Rice Galingale) and the dicotyledonous weeds (Copperleaf, Common Carpesium, and Yerbadetajo). The dataset consisted of 84 digital color images (35 of corn and 49 of weed). These images were then converted to gray-level images with the use of the color index, ExG -ExR followed by which wavelet transform was used to extract features. Two-level wavelet transform was performed to extract the following components: approximation component, A2, detail components namely, H1, V1, D1, H2, V2, D2, and energy values namely, e A2, e H1 e V1 e D1 e H2, and e D2 . These energy values were then input to a BP network which was able to separate the weeds species with an accuracy of 100% but not the corn and weeds. When the energy features were used as input vectors, it resulted in a classification accuracy of 77.14%, whereas when the fractal dimensions of the images were used, it resulted in a classification accuracy of 80%. Interestingly, when both the fractional dimensions and energy features were input to the network, they obtained an improved classification accuracy of 94.28%. They also used shape parameters in the BP network for the identification of weeds .",
          "table": []
        },
        {
          "text": "Chen et al.  proposed a method to classify monocotyledonous and dicotyledonous weeds (species not mentioned) in a corn seedling using shape features. The dataset used for the task included 60 images of corn and 280 images of weeds, the nature or format (RGB or grayscale) of the image was not mentioned. The background was removed using Otsu's threshold based on the Excess green method. 20 images of corn and 80 of weed were used for training a PNN while 40 images of corn and 200 of weed were used for testing purposes. The PNN made use of the following shape features: area ratio, aspect ratio, eccentricity, and roundness. From the confusion matrix, it was evident that 37 images of corn and 190 of weed were correctly classified, resulting in an accuracy of 92.5% for corn and 95% for weed. The authors cited the time of image gathering as a possible reason for the misclassification i.e., gathering of images in the early stages of growth could have yielded better results. The performance of the PNN when compared with that of a BP network trained and tested on the same dataset, remained superior as the BP network could only yield accuracies of 87.5% and 93%, respectively for corn and weed.",
          "table": []
        },
        {
          "text": "Another wavelet-based application was demonstrated by Sajad et al.  where wavelet analysis using a two-dimensional DWT extracted the appropriate features for classification (identifying weed in corn) using an ANN. The dataset consisted of 35 corn images and 50 weed images.",
          "table": []
        },
        {
          "text": "The weeds involved in the study were: Common lambsquarters, Alhagi maurorum, Convolvulus arvensis L, and Amaranthus. For training purposes, 20 corn and 30 weed images, respectively were used while for testing purposes, 15 corn and 20 weed images were used. The images were segmented using Excess green index followed by which DWT was used to extract the following features: energy, entropy, inertia, contrast, and local homogeneity. The ANN produced a classification accuracy of 98.8%. Andrea et al.  demonstrated the use of CNNs for maize and weed (the types/species of weed were not mentioned) classification. The CNN architectures used were AlexNet, sent, cNet, and LeNet, and their performances were analyzed. The dataset consisted of 2835 RGB images of maize and 800 of weed. These images were segmented through Otsu's thresholding to remove the unwanted elements like soil and other non-plants elements thereby separating the plant i.e., the target object from the background. To reduce overfitting and improve precision, the images were rotated every 30 • thereby resulting in an increased dataset of 34,020 images of maize and 10,560 images of weed. 25,965 images of maize and 8560 of weed were used for training while 8325 of maize and 2000 of weed were used for validation purposes. When the training performances of all four CNN architectures were compared, cNet gave the best performance with an accuracy of 96.40%. Further, when cNets with 64 filters and 16 filters, respectively were compared with each other, cNet with 16 filters gave a superior performance of 97.26%. Another dataset (information not mentioned) consisting of 202 images of maize and 202 images of weed was used for testing purposes. The performances of both the cNets were compared on the following hardware: CPU, CPU with Raspberry Pi 3, and GPU. cNet with 16 filters gave the best accuracy of 92.08% for maize and 89.11% for weed, respectively and an average classification time of 1.58 ms for GPU. It gave exactly similar accuracies for the other two hardware too.",
          "table": []
        },
        {
          "text": "Another application of CNN was demonstrated by Dellia et al.  where CNN was used to discriminate i.e., object detection between weeds and maize using the context surrounding the images. The weeds studied were Grass (foxtail) and grass-like (yellow nutsedge). 224 aerial images of maize fields comprised the dataset which was divided into three categories: training (158), validation (33), and testing (33). The next step was to split the images into smaller ones of size 300 × 300 pixels by placing a grid of 300 × 300 pixels over the larger images. This was followed by manually labeling them as weed and non-weed thereby resulting in 8 times more images of non-weed than weed. Therefore, a data augmentation technique (not mentioned) was used to augment the existing weed images (number not mentioned). The dataset was then re-split into training, validation, and testing, respectively. Based on the idea of adding context (adding a 300-pixel border to the surrounding of the central square image of 300 × 300 pixels), two more datasets were created namely, a dataset consisting of rectangular images with full-stretched context and a dataset consisting of square images with edge-stretched context. While the former was created by looking at the central image of 300 × 300 pixels and stretching any of its sides that did not possess a border of 300 pixels to the edge of the full-sized image, the latter was created by stretching the sides to 300 pixels only. Therefore, the former comprised rectangular images while the latter comprised square images. CNNs were implemented 3 times on each of the 3 datasets and each set of the three runs was averaged and their validation accuracies were compared. The results read: validation accuracy of 94.6% for no context, 97.1% for edge-stretched and 96.3% full-stretched, respectively. The best performing model was the edge-stretched context model which was tested on the test dataset and its performance was compared with the no-context model. The results read: accuracy (92.9% for no-context model and 95.7% for edge-stretched context model), precision (61.9% for no-context model and 75.5% for edge-stretched context model) and recall (88.5% for no-context model and 88.6% for edge-stretched context model). Heatmaps of the original drone images were created using the trained model following which the ES-context model was subject to a comparison with a human baseline (created by the Turkers drawing boxes around all the weeds detected by them in the image). A comparison was made between these responses and the heat maps created by the CNN. It was observed that the CNN could detect 150% more weed patches than the Turkers.",
          "table": []
        },
        {
          "text": "Drymann et al. [60] demonstrated a pixel-wise classification of crops and weeds by using a CNN which is a modified version of VGG16. The crop involved in the study was maize and the type/species of weed were not mentioned but it was stated that the weeds belonged to 23 different species. By randomly placing segmented plants on top of soil images, simulated field images were created and used for training a fully convolutional neural network. Ground truth segmented images where each pixel is labeled as a weed (marked as blue), soil (marked as red), or plant (marked as green) were used for creating simulated images. 8340 and 301 images of the segmented plants and soil, respectively were used for generating modeled images. Training data was generated by using 80% of the plants' images while testing data was generated by using the remaining 20%. The images were then resized to 800 × 800 pixels thereby resulting in a dataset of 3463 images for training and 123 for verification. The performance of this plant segmented method was evaluated on 2 real images which were segmented by hand, one gathered in a healthy maize field, where the plant overlap is little and, another from a maize field possessing smaller maize plants and a higher weed coverage. For both the images, the algorithm successfully detected both the crop and weeds, with the classification accuracy for the first image being 98.3% and for the second, being 94.4%. Citing that this accuracy does not consider that there are a lot more soil pixels than that of crop or weed, the authors considered a metric, IOU, for each class. For the first image, IOU was 0.93, 0.98, and 0.79 for the crop, soil, and weeds, respectively and for the second image, IOU was 0.71, 0.93, and 0.70 for the crop, soil, and weeds, respectively. Summaries of studies that employed Neural Networks for the identification of weeds summarizes all the above works that employed neural networks for the identification of weeds.",
          "table": [
            "Summaries of studies that employed Neural Networks for the identification of weeds."
          ]
        }
      ]
    },
    {
      "title": "Miscellaneous models for the identification of weeds",
      "div": [
        {
          "text": "Apart from SVM and NNs, other ML approaches were also made use of for the identification of weeds in corn, this section describes all such works. The same authors  used DA and DT for the same classification problem . Among the 71 wavebands available, the most important bands were extracted using SAS software's STEPDISC feature. Here again, there were three classification problems involved: just the three nitrogen application rates, just the three weed treatments, and the nine weed and nitrogen treatment combinations. The STEPDISC approach was applied for all the three classification problems thereby resulting in the following sequence of bands for the three growth stages namely, early stage of growth, tasseling stage, and fully grown stage: bands 34-42-42, bands 26-32-31, and bands 28-19-19, respectively. DT chose a smaller number of wavebands for each of the three classification problems. Risk estimate values were derived by dividing the number of instances that were categorized incorrectly by the number of cases that were used in the classification. A lower risk estimate denoted a high categorization accuracy. When the performances of DA and DT were compared with that of ANN, it was observed that the DA provided a classification accuracy of 75% while ANN and DT only gave 58% and 60%, respectively for the first classification problem. For the second problem, the classification accuracies were: 87%, 76%, and 68%, respectively while for the third problem, the accuracies were: 83%, 81%, and 69%, respectively. While DT produced the greatest classification accuracy of 71% for the first classification problem at the tasseling stage, the ANN approach produced the most accurate results of 88% each for the other two classification problems. DA produced the best results for the combined case at the fully-grown stage (79%). Furthermore, the ANN gave the best results for the other two cases: weeds (85%) and nitrogen (88%). And for the early stage of growth, DA performed best for all three classification problems. An early work by Hossein et al.  demonstrated the real-time classification of the weed, Amaranth (pigweed) in corn with the use of FFT. The images were obtained from cornfields containing the weed. They were then preprocessed for color segmentation (Euclidean distance algorithm was applied to the red and green values of each pixel), conversion to grayscale, and detection of edges based on the difference in gray intensities between two adjacent pixels. The segmented images were divided into the background, crop, and weed based on frequency, and density using two-dimensional FFT. Followed by this was the post-processing stage where regions of the images were re-checked to check for misclassifications and finally combined into a single image. The above stages were performed for the removal of the background and classification of the plant. When the performance of the proposed method was tested on 80 cornfield images, the results showed that FFT produced a classification accuracy of 92.8%. The authors also mentioned the possibility of using the proposed method on a cultivator robot that would be equipped with a digital camera to capture images following which the classification would be performed and finally the weeds will be removed by the use of herbicide sprayers, cutting blades, etc.",
          "table": []
        },
        {
          "text": "Gee et al.  proposed a method for real-time weed control by discriminating between crop and weed and estimating the inter-row WIR. The crops involved in the study were sunflower, maize, and wheat while the type/species of weed was not mentioned. Two types of images were involved in the study: 300 agronomic images (50 images each with WIRs of 0%, 10%, 20%, 30%, 40%, and 50%, respectively) which were created by using a simulation engine and, 100 plus wide-view angle RGB images acquired from fields of sunflower (35 images), wheat (35 images), and maize (30 images) followed by processing in Matlab 6.5 software. On the RGB images, Excess Green thresholding was performed. Neither the angle of the light source with respect to its target surface nor its intensity seemed to affect the normalized RGB coordinates thereby resulting in just the green channel being considered. This was followed by the detection of crop rows using a DHT to detect only the crop rows present in the picture. Classical blob coloring analysis, a region-based segmentation technique was then used to discriminate between crop and weed. Based on spatial similarity and color, it grouped the linked pixels into areas. Given that identified lines were classified as crop, it was determined that if a pixel of a straight line was observed to be belonging to an area, it should be classified as crop, otherwise it should be classified as weed. When the algorithm was tested on simulated images, it gave the following results: 100%, 100%, 94%, 92%, 89%, 82% crop row recognition rates for WIRs of 0%, 10%, 20%, 30%, 40% and 50%, respectively. When tested on in-filed images, the algorithm was able to recognize crop rows that had low WIRs i.e., 0 to 10% with an accuracy of 88%. To differentiate between crop and weed, a comparison was made between the detected WIR and the WIR initially fixed in the images that were simulated and the in-filed images that were segmented manually. When the algorithm was tested on 5 images of maize with medium WIR, the detected WIRs were 32.47, 18.74, 22.97, 15.96, and 12.47, respectively while the manual WIRs that were estimated were 22.8, 19.16, 18.17, 17.36 and 29.24, respectively.",
          "table": []
        },
        {
          "text": "The same authors proposed an approach based on wavelet transforms for the discrimination of crop and weed . Again, two types of images were involved in the study: 1530 gray synthetic images created by using a simulation engine and RGB images (number not mentioned). The following three possible image configurations were modeled for synthetic images based on the distinct weed spatial distributions: Punctual (Poisson process), Aggregative (Neyman-Scott process), and a combination of both thereby resulting in thirty series of images. Followed by this, 17 synthetic images were created for each series using different WIR inter-row (inter-row WIR) values ranging from 0 to 80%. The RGB images were then binarized using k-means clustering. 33 wavelet transforms from 6 wavelet basis functions (Daubechies, Symlet, Coiflet, biorthogonal, reverse biorthogonal, Meyer) were analyzed following which, the best two (Daubechies 25 and the Meyer) and the worst one (Biorthogonal 3-5) wavelets were chosen. Crop rows were detected using a real bi-dimensional Gabor filter (a cosine signal modulating a Gaussian function). Through analysis of the confusion matrix and comparison between the resulting and initial WIRs, a comparison was made between Gabor filtering and the three wavelet transforms. The terminologies used with respect to the confusion matrix were: False Crop (FC), True Crop (TC), False Weed (FW), and True Weed (TW). Using these terminologies, the followed were calculated: Initial WIR (Initial WIR inter-row ), Initial Crop Rate (Initial CR), Detected WIR inter-row , Detected CR, True Weed Detection Rate (TWDR), True Crop Detection Rate (TCDR), and the error rates for the false detection of crop and weed. For the synthetic images, the best results were obtained for the punctual image configurations: Meyer (overall accuracy of 89.4% and weed error percentage of 2.8), Daubechies 25 (overall accuracy of 89.7% and weed error percentage of 2.8), and Gabor filtering (overall accuracy of 83.3% and weed error percentage of 14.9) thereby implying the superior performance of wavelet transforms. For the real images (i.e., the ones captured in RGB and binarized using K-means), the results read: Meyer (overall accuracy of 80.6% and weed error percentage of 5.8), Daubechies 25 (overall accuracy of 80.7% and weed error percentage of 5.8) and Gabor filtering (overall accuracy of 76.3% and weed error percentage of 18.5) thereby implying the superior performance of wavelet transforms again.",
          "table": []
        },
        {
          "text": "The work by Asif et al.  presented a vision guidance system for an automated robot that is used for weed detection. k-means was then used for color segmentation followed by which ROI was automatically selected, the images were converted to grayscale, and edges were detected using Sobel edge detection. Hough Transform was then used to detect the crop boundaries and depending on its success, the robot was assisted in following the crop boundaries. If the HT did not detect the crop boundaries a certain number of times, then the ROI was widened. The tracking parameters, which indicated the orientation and location of the crop borders with respect to the image's center, were obtained with the aid of HT. The developed system also aided in the successful detection and tracking of the crop boundaries. The errors on the synthetic images were less than ± 5 pixels and ± 10 • for translation (the robot's current displacement with respect to the reference position) and orientation, respectively. The authors also mentioned that the errors can be minimized further with the use of appropriate estimators like the Kalman filter algorithm and particle filtering algorithm. Longchamps et al.  investigated the ability of LDA to classify maize and weeds using their UV-induced fluorescence. The plants studied were Corn hybrids namely, Monsanto DKC 26-78, Syngenta N2555 and Elite 60T05, and monocot/grass hybrids namely, Echinochloa crus-galli (L.) Beauv., Digitaria ischaemum (Schreb.), Setaria glauca (L.) Beauv. And Panicum capillare (L.), and dicot hybrids namely, Ambrosia artemisiifolia (L.), Chenopodium album (L.), Capsella bursa-pastoris (L.) Med and Amaranthus retroflexus (L.). 1440 spectral signatures of fluorescence were obtained from three experiments that were similar, performed at three different times. Some spectra were missing and hence, only 1361 spectra were available and the most important information from these was obtained using PCA. Using the plant species, the first five principal components as inputs, and cross-validation, LDA was performed. The output was a confusion matrix with a prediction error of 37%. A second classification was performed by combining the hybrids into groups thereby resulting in a confusion matrix with a prediction error of 8.2%, indicating a classification accuracy of 91.8%. 388 spectra of Grasses were correctly classified as Grasses, 9 and 49 were misclassified as Dicots and Corn, respectively. 423 spectra of Dicots were correctly classified as Dicots while 17 were misclassified as Grasses. 439 spectra of Corn were correctly classified as Corn while 36 were misclassified Grasses. The author cited the reason for the prediction error as the confusion between Pioneer 39Y85 and Setaria glauca L. (Beauv.) as Corn and Grasses belong to the same taxonomic family, Poaceae.",
          "table": []
        },
        {
          "text": "Xavier et al.  demonstrated a Computer Vision system that uses real-time image processing to identify (classify and detect) weeds in maize. The weeds involved in the study were: Datura Stramonium L., D. Ferox, S. Halepense, and X. Strumarium. The data used in the study consisted of 6 videos with each one having an average duration of 12 s or 300 frames thereby resulting in a total of 1800 frames. Segmentation according to the threshold was done to obtain binary images where the pixels corresponding to the vegetation were separated from the ones corresponding to non-vegetation. The real-time image processing method/system involved two sub-systems namely, RCRD and FIP that were independent of each other but worked simultaneously. RCRD was used to detect pixels corresponding to the crop rows pixels. Using an AND operation i.e., producing an image with only the persisting vegetation pixels being retained, RCRD merged all the binary frames thereby resulting in a single image. And for the ones (images containing large patches of weed) for which the AND alone was not enough, the RCRD created crop rows to be used by the FIP. The crop row pixels of the image i.e., of crop rows that coincided with the group of positions that were marked by the FIP was preserved while the rest were discarded. The system was tested on many videos of maize obtained from different fields over different years, detecting an average of 85% of weed and 69% of the crop. It not only performed well under different conditions like varied illumination, soil humidity, and blurred conditions but also when it was presented with very difficult growth stages of crop and weed. When tested on good images i.e., the ones that had clearly visible crop rows, the system performed well by producing an average classification accuracy of 95% for weed and a classification accuracy of 80% for crop. It also always maintained a very low rate of false negatives for weed (1%).",
          "table": []
        },
        {
          "text": "The work by Liu et al.  was an object detection problem that used SVDD for the recognition of weed or corn. Seventy-five sub-images (256 × 256 pixels) of corn and 43 sub-images (256 × 256 pixels) of weed were extracted from originally gathered RGB images. The type/species of weeds were not mentioned. The excess green index was used to convert these into gray and binary ones following which the next task was to perform wavelet decomposition to extract morphological features and energy-based features. In order to do so, a two-dimensional multi-resolution analysis was performed on the images to separate frequency components (both low and high). Discrete didactic wavelet transform was used to decompose images into the following four component groups: high-frequency components namely, H1, D1 (in x, y, and xy directions), and low-frequency component A1. Further decomposing A1 resulted in four more components of lower resolution namely, A2, V2, D2, and H2. The energy percentages of these 7 components were considered and the energy features were derived. 3 morphological features: Shape complexity indicator (C), Elongation factor (D), and Thickness (T) were selected. To further select better features, single-features SVDD models with these 10 features were constructed on the following dataset: 40 images of corn and 10 of weeds in the training set and 35 images of corn and 10 of weeds in the test set. The model's performance was evaluated by using RATE as the metric where RATE was defined as the number of correctly classified objects divided by the total number of objects. The SVDD obtained when T is the input vector, gave the best performance with a testing rate of 88.2%. For further analysis, the three morphological characteristics and five wavelet-based features associated with a RATE>60% were chosen, and every possible combination of these features was utilized to create SVDD models. The four best performing SVDD models were: SVDD (e V2, T), SVDD (e H2, e V2, T), SVDD (e H2, e V2, C, D) and SVDD (e H2, e V2, C, D, T) with RATEs of: 94.12%, 95.59%, 94.12% and 95.59%, respectively. SVDD (e H2, e V2, T), and SVDD (e H2, e V2, C, D, T) performed the best but considering the fact that the accuracy might be reduced if more features are considered, SVDD (e H2, e V2, T) was chosen as the best multi-feature based SVDD. This model when compared with SVM and FLDA, proved superior in terms of performance even when the number of weed samples for training was gradually reduced from 25 to 5. The results read: SVM (75.17%), FLDS (66.04%), and SVDD (94.34%).",
          "table": []
        },
        {
          "text": "The work by Montalvo et al.  demonstrated a method for the detection of crop rows in maize fields that contained high weed pressure (the type/species of weed was not mentioned). The image data consisted of 300 RGB images of maize (the first set of 200 images with high weed densities and the second set of 100 images where the weed density is exceptionally high). These images were transformed to grayscale using Excess green index following which a double Otsu approach was applied to separate the crop and weeds. The equations of straight lines associated with the crop rows were then computed using a linear regression technique that was based on total least squares. The performance of LR was compared with that of HT over images of various resolutions (1390 × 1044 (first set of images), 696 × 522 (first set of images), 720 × 576 (second set of images), and 360 × 288 (second set of images)). The former outperformed the latter for every resolution mentioned above, with the best performance (percentage of effectiveness) being: LR (95.5%) over HT (89.3%) for the images with a resolution of 1390 × 1044.",
          "table": []
        },
        {
          "text": "Rainville et al.  presented a computer vision-based weed/crop classification using morphological analysis. The crops considered were corn and soybean and the type/species of weed was not mentioned. The dataset consisted of 149 RGB images of corn and soybean. The images were segmented using PCA (used to extract the components that are associated with the vegetation) and Otsu thresholding (used to find the threshold to discriminate between the vegetation and soil). The crop rows were first identified using HT followed by which row borders were determined by analysis of the plants/vegetation that crosses the center lines of the row (these plants were considered a weed). Based on the data provided by these, PDFs of the weeds' morphological characteristics (area, compactness, major axis) were computed. Considering that the plants that were present inside the rows were a mixture of both weed and crop, PDFs were deduced by utilizing the data from inside the rows.",
          "table": []
        },
        {
          "text": "Following this, Naïve Bayes was used to discriminate the crop and weed that were present inside the rows. The data that was classified as the crop was then sent to GMM, another classifier that identified any leftover weed i.e., the weeds that were wrongly classified as crop. Any newly identified weeds were added to the list of previously classified weeds. As a measure of demonstrating classification accuracy, the authors worked on the sub-set of images (129 of the 149 images) for which the row positioning using Hough Transform was a success. The combination of the Bayesian classifier and GMM gave the following global classification performance average for the three morphological characteristics: area (90.8%), compactness (89.9%), and major axis (90.8%) with standard deviations of 4.4, 5.1, and 4.5, respectively.",
          "table": []
        },
        {
          "text": "The potential of a LIDAR sensor to detect crop rows, as well as maize and weeds was evaluated by Dionisio et al. . The weeds involved in the study were: Galium aparine L., Lamium purpureum L, Veronica persica Poir, and Echinochloa crus-galli (L.) P. Beauv. The distance and reflection measurements for the vegetation were gathered using the LIDAR sensor followed by which the real heights of the plants were evaluated. Compared with the RGB images collected from the same plants, a strong correlation of 0.75 was observed between the heights measured by the LIDAR sensor and the real height of the pants. The system's ability to distinguish between plant and soil was determined using regression analysis for which the following two scenarios were considered: (i) the LIDAR's geometrical reliability (ii) the system's capacity to distinguish between the presence and absence of vegetation (logistic binary regression was used for this purpose as the presence/absence of vegetation is a binary variable). For the LIDAR measurements, the logistic regression performed very well, demonstrating a high level of accuracy in predicting the presence/absence of vegetation. For a total of 1558 sample units, it was observed that the predicted accuracies for logistic regression read: vegetation (95.3%) and soil (82.2%). To distinguish between soil and vegetation and also between plants and weeds, CDA was used. Although the CDA was more successful than the logistic regression in separating the plant from the soil, it had a lower success rate. With an overall accuracy of 72.2%, CDA was able to distinguish between the following four sorts of classes: monocots, dicots, maize, and soil. The accuracies were: 92.4% for soil, 64.5% for dicots, 34.5% for monocots (accuracy was low since it was classified as crop), and 74.3% for maize.",
          "table": []
        },
        {
          "text": "The work by Amir and Ali  introduced a weed control robot capable of identifying weeds in cornfields by classifying pixels into corn and weed. The dataset consisted of 73 images taken in a cornfield, the type/species of weed was not mentioned. In order to extract plant pixels from the original images, the Excess green method was used followed by auto-clustering. The hue plane of the image was extracted following which clustering was applied to it, and some morphological operations was then performed. The spatial and frequency features of images were extracted by using wavelet transform and were further used for classification. The entire method was implemented in LabVIEW software, which classified 70 of the 73 images thereby resulting in an accuracy of 95.89%. After the weed regions were identified, a hardware interface was used to send commands to the nozzles of the robot to spray herbicides.",
          "table": []
        },
        {
          "text": "Shubham  demonstrated a method for the classification of weeds in a maize field. The dataset involved in the study consisted of 60 RGB images of a maize field, the type/species of weed was not mentioned. These RGB images were transformed to gray ones using Excess green index followed by which the weeds and crops were separated using a double thresholding Otsu approach. It was then extended across each crop row for the weed present in that crop using the PCA technique to identify crops from weed in high-density regions. PCA was able to classify 55 of the 60 images correctly as weed or crop thereby resulting in an accuracy of 91.67%.",
          "table": []
        },
        {
          "text": "Pantazi et al.  proposed a new active learning approach to discriminate between maize and different species of weed based on the variations in their spectral reflectance. The weeds involved in the study were: Ranunculus repens, Urtica dioica, Medicago lupulina, Poa annua, Cirsium arvense, Oxalis europaea, Stellaria media, Sinapis arvensis, Tarraxacum officinale, and Polygonum persicaria. Spectral features were extracted using a hyperspectral optical sensor that was mounted on a robotic platform. Through reflectance calculation, plant selection, NDVI, and spatial resolution and, spectral analysis, the following spectral bands were chosen: 550, 580, 660, and 830 nm. The weeds were recognized and rendered outliers using the one-class classifiers and then added to a new multi-class classifier that detected any new species of weed that appeared. This procedure was repeated until the multiple class classifier had all weed classes i.e., all the weed classes were augmented in the multi-class classifier. The rest of the method was repeated for the following weed type, ensuring that the suggested technique can learn and enhance any new weed species forever. SVMs, autoencoders, MOG, and SOM were the ML methods used. Feature selection was performed on 110 spectra pertaining to maize plants thereby resulting in a dataset of 110 samples each of which had a vector of features. The one-class classifiers were then tested to recognize the new species as outliers by using a total of 54 additional samples belonging to maize plants and 54 from a single weed species. Crop and the outlier spectra obtained from one species of weed were used as the baseline set in the next phase. The process of outlier detection was a repetitive one involving the addition of the first weed species to the data samples pertaining to the crop. Following that, the one-class classifiers were fed with a new weed species as well as the data from the already existing crop and weed species. If a new sample was discovered to be from one of the baseline set's crop or weed species, the outlier detection procedure had to be run for each class inside the baseline set, and the sample was classed as belonging to one of the baseline sub-classes. This process was iteratively repeated for every newly appearing weed species. While for crop recognition, the one-class MOG and one-class SOM achieved a rate of 100%, the best recognition rates for weed species read: 98.15% (MOG) and 98.44% (SOM) for Cirsium arvense, 90.74% (SOM) for Sinapis arvensis, 94.44% (MOG) and 92.59% (SOM) for Stellaria media, 90.74% (SOM) for Tarraxacum officinale, 94.44% (SOM) for Poa annua, 94.44% (SOM) for Polygonum persicaria, 94.44% (SOM) for Urtica dioica and, 94.44% (SOM) for Medicago lupulina. Good recognition rates of 83.33%, 79.63%, and 85.19% were also obtained for Poa annua, Tarraxacum officinale, and Medicago lupulina, respectively while SOM provided a good rate of 85.19% for Oxalis europaea and the autoencoder gave a good rate of 83.33% for Medicago lupulina.",
          "table": []
        },
        {
          "text": "The integration of texture, shape, and spectral characteristics to classify crops and different species of weed was investigated by Zhang et al. . The crop studied was corn and the weeds were dicotyledonous weeds namely, lobed leaf pharbitis (Pharbitis nil L. Choisy), redroot amaranth (Amaranthus retroflexus L.), leaf pharbitis (Pharbitis purpurea L. Voigt) and purslane (Portulaca oleracea L.) and monocotyledonous weeds namely, green foxtail (Setaria viridis L.) and goose grass (Eleusine indica L.). SPCA was used to select the following relevant wavelengths: 710, 516, 843, 677, and 749 nm. For classification, the following texture, shape, and spectral characteristics were employed using C 5.0 algorithm: texture features namely, homogeneity, entropy, and contrast for the wavelengths 677, 843, and 516 nm, shape features namely length, width, shape index and area and, spectral features namely, ratio of the bands 677 and 710 nm, normalized difference index between the bands 749 nm and 710 nm, red index, and ratio vegetation index. The model produced both a global accuracy and kappa coefficient of over 95% when spectral and shape features were used.",
          "table": []
        },
        {
          "text": "The work by Gao et al.  looked into the potential of classifying weed and maize using hyperspectral imaging. The weeds in the study were: C. arvensis, Rumex, and C. arvense. The dataset consisted of hyperspectral images of each of the three weeds and 25 of maize. ROIs of the leaves of the plants involved in the study were used and consisted of 79, 80, 80, and 84 for C. arvensis, Rumex, C. arvense, and maize, respectively. For each band, the calibrated reflectance of ROIs was calculated followed by which 80 NDVIs (ranging between 0 and 1) and 80 RVIs were calculated and used for feature construction. When these were subject to PCA, the redundancy was reduced and only the first five principal components were used for further analysis. Classifiers with different combinations of spectral features were built using RF. Following feature construction, importance scores determined the 30 most important features to be chosen by an accuracy-oriented feature reduction technique. Five-fold cross-validation (one set for testing and four for training) was used to evaluate RF. The results read: Maize (94% precision and 100% recall), Rumex (70.3% precision), C. arvense (65.9% precision) and C. arvensis (95.9% precision). When the performance of RF was compared to that of KNN using a McNemar test, the optimal RF model performed better than KNN at a significance level of 0.05.",
          "table": []
        },
        {
          "text": "Zheng et al. [76] developed an efficient method to classify maize and weeds by using color features. RGB images of weeds (species not mentioned) and maize were gathered for the three consecutive years 2011, 2012, and 2013 and consisted of three classes: maize, weed, and soil. The images were then preprocessed for background removal using Excess Green followed by Otsu's thresholding. PCA aided in the extraction of the following color features: single-color indices namely, Rn and Gn, two-color indices ExR and three-color indices namely, ExR, ERI, Gray, EGI, and CIVE. SVDD was used for classification and was trained on 197 image samples from the year 2011. 4333 samples of maize, 5730 of weed from 2011, 3573 samples of maize and 7976 of weed from 2012, and 1465 samples of maize and 7878 of weed from 2013 were chosen for testing purposes. When compared with LS-SVM, SVDD models performed better on the data for all three years based on the results evident from the confusion matrix. The results read: LS-SVM (accuracies of 89.08%, 87.87% and 90.44%, respectively) and SVDD (accuracies of 90.19%, 92.36% and 93.87%, respectively). Citing lower sensitivity to canopy overlap, wind effect, leaf orientation, etc., the authors concluded that the use of color indices for classification is more practical than the use of shape/texture features. Summaries of studies that employed miscellaneous models for the identification of weeds summarizes all the above works that employed ML techniques other than SVMs or neural networks for the identification of weeds.",
          "table": [
            "Summaries of studies that employed miscellaneous models for the identification of weeds."
          ]
        }
      ]
    },
    {
      "title": "Dataset size, data augmentation, transfer learning, and performance metrics",
      "div": [
        {
          "text": "Training a new classification model with the desired accuracy may require huge amounts of data and sophisticated computational tools to process the collected data. The size and quality of the dataset have a major impact on the image classification performance of ML models. In general, smaller datasets result in poor classification accuracy. This has been shown in numerous previous studies . The size of the dataset necessary for the desired accuracy is determined by a variety of criteria, including the number of classification categories, the complexity of the features present in the images that are to be classified, and class imbalance. For example, Luo et al. showed that an image dataset size of 6000 was required when the number of classification categories were 3 and when the categories was increased to 8, a dataset size of 40,000 was required to obtain 90% accuracy . Smaller datasets also result in the overfitting of the model . Overfit models tend to memorize the variance along with the underlying relationships and perform poorly on the testing and unseen future datasets . Yet, in our domain, it is often too expensive or not viable to build a large dataset for training.",
          "table": []
        },
        {
          "text": "The problem caused by the smaller datasets (especially overfitting) can be partially mitigated through data augmentation . Data augmentation expands the original dataset through image manipulations, feature space augmentation, and adversarial training . Image manipulations include geometric transformations, color space transformations, random erasing, introducing corruptive noises, and image mixing. Geometric transformations involve the modification of geometrical features through flipping, translation, rotation, and cropping of images . Color space transformations use filters to modify the RGB space of images . Random erasing is about randomly removing some of the image features . In image mixing, new images are formed by combining the pixel values of pair of images . In contrast to all the image-based approaches discussed above, Devries and Taylor introduced a novel way of augmenting data in the feature space . Instead of applying transformations to the input, they have applied transformations to the encoded versions of the inputs. Another interesting approach to data augmentation is the use of GANs . GANs consist of two different competing networks called generator and discriminator. The role of the generator is to confuse the discriminator that the artificially generated image is real whereas the discriminator's role is to differentiate the real images from the synthetic images created by the generator. The simultaneous training of these two networks generates artificial images which share similar characteristics to original images.",
          "table": []
        },
        {
          "text": "Though data augmentation offers a way to increase the size of the dataset to mitigate the overfitting of the model, it is computationally expensive and time-consuming. Another popular approach that allows us to bypass the requirement of a larger dataset is transfer learning . Instead of developing a neural network (say for image classification) from the ground up, we can utilize the learned parameters of a neural network trained on different yet related domain tasks to train our new task of interest. This approach is successfully applied in many domains including precision agriculture. For instance, Espejo-Garcia et al. leveraged the trained neural network parameter of ImageNet for weed classification . Suh et al. again utilized the pre-trained network on ImageNet dataset for the classification of sugar beet and volunteer potato . Kaya et al. compared four different transfer learning models with the one developed from scratch for plant classification . Currently there are various approaches available in transfer-learning and can be found elsewhere .",
          "table": []
        },
        {
          "text": "We have discussed the importance of larger datasets for higher classification accuracy. But classification accuracy is just one of the parameters to evaluate the model's performance and in many situations, accuracy alone is not sufficient enough to quantify the performance of an ML model. The other commonly used performance metrics are F1score, precision, confusion matrix, recall, log-loss, and ROC-AUC [107]. The accuracy of a classifier is defined as the ratio of the number of correct predictions to the total number of predictions. But it gives misleading performance evaluation when the dataset is imbalanced. In order to understand how to utilize other performance metrics, we need to understand some basic terminologies involved such as true positives, false positives, true negatives, and false negatives. The definitions of these terms are summarized in Definitions of a classifier predictions. The mathematical definitions of commonly used performance metrics are given in Definitions of a classifier predictions.",
          "table": [
            "Definitions of a classifier predictions.",
            "Mathematical definitions of performance metrics. Note: pprobability of an instance belonging to a class."
          ]
        },
        {
          "text": "Though the confusion matrix is not strictly a performance metric, it gives an overview of positives and negatives predicted by the classifier. An illustration of the confusion matrix for binary classification is given in Fig. 2. Precision measures the correctly predicted positives out of the total positives predicted. Higher precision indicates a smaller number of false positives in the model and is used in situations where the false positives in a model are highly undesirable. Recall or sensitivity or true positive rate talks about the ability of the model to predict the true positives out of total real positives. A higher recall is important in the identification of diseases. F1 measure gives equal weightage to the false positives and false negatives and a harmonic mean of recall and precision. Miss rate or false positive rate is the additive inverse of sensitivity and measures the proportion of correctly identified negatives with respect to the total predicted negatives. Log-loss measures the deviation of the predicted probability of an instance belonging to a class to that of the actual probability (in general 1) and is commonly used in binary   A Perfect classifier has a true positive rate of 1 for all classification thresholds. It is to be noted that unlike other performance measures ROC-AUC is an index measure. Though ROC is mainly developed for binary classification, it can be extended to multi-class classification by clubbing the rest of the categories as one class. A detailed review of error metrics can be found here . Performance metrics alone is not sufficient to choose an appropriate model for the problem at hand, the computational burden of the models in terms of time and space complexity must also be investigated for a viable application of the model in the practice.",
          "table": []
        }
      ]
    },
    {
      "title": "Conclusion and future research directions for the identification of weeds in corn",
      "div": [
        {
          "text": "In this study, we analysed 35 research studies pertaining specifically to weed detection in corn. Of these 35 research studies, twenty-seven studies involve classification problem while seven addresses the problem of object detection. There were also two articles that solved both these problems. Fig. 4 presents the distribution of these articles according to the type of ML problem. Three ML approaches were used namely, SVM, Neural Networks and Miscellaneous (Bayesian networks, Decision Trees, Genetic Algorithms etc.). Eight of the presented articles used SVM as the ML approach, ten used Neural Networks, and seventeen used Miscellaneous approaches (Bayesian networks, Decision Trees, Genetic Algorithms etc.). Fig. 5 depicts the distribution of these articles according to the ML approach used. On further analysis, it was found that the SVM with RBF kernel function was the most used type of SVM while BP network was the most used Neural Network although CNNs were also commonly used. Among the Miscellaneous approaches, wavelet transforms were the popular ones while DT, PCA, customized Computer Vision systems etc., were also used. The most popular type of data that was used by these approaches was color data (images and videos) comprising twenty-eight of the presented articles while in eight articles, spectral data (hyperspectral data, reflectance spectra, fluorescence spectra etc.) was also used. The distribution of articles based on the type of data involved is shown in Fig. 6. A variety of weeds belonging to different categories namely, broadleaf, narrow leaves, grassy, dominant grassy, dominant broad-leaves, monocotyledonous, dicotyledonous etc., were involved in the articles.",
          "table": []
        },
        {
          "text": "Industry, university academics, and the USDA (United States Department of Agriculture) are all working together to advance the identification of weeds in corn. Based on our review more efforts should be directed to the following areas:",
          "table": []
        },
        {
          "text": "(1) Data acquisition and augmentation: Data plays an important role in aiding in the identification of weeds. Of all the studies reviewed, only two studies made use of data augmentation and some works were observed to be using minimal to very little data for the training. This might not suffice for the task of identifying weeds, and most importantly, the results obtained by using low amounts of data do not justify the predictions of the trained algorithms. (2) Early identification of weeds: Although most works in this review dealt with the data acquired when the weeds were in the early growth stages, some works dealt with data that was    collected when the weeds were in the later stages of growth i.e., three weeks, six weeks or more. This is a drawback because, at three to six weeks, weeds will display distinguishable features that can aid in manual identification relegating the need for identification through algorithmic means. models and offer little explanation on how the predictions are made. This lack of transparency questions the reliability of the predictions or conclusions made by the ML models. Hence, the interpretability of ML models used for the identification of weeds is highly desired to understand the embedded biases in the network and also to identify the important input features and conditions that led to the ML decision. Although some research  focused on interpretability of neural networks employed for identification of weeds, more works are needed to understand the bias in the network and the important input features responsible for the weed identification and classification.",
          "table": []
        }
      ]
    }
  ],
  "tables": [
    "{\"title\": \"List of abbreviations. artificial neural network\", \"head\": [\"active shape models\"], \"value\": [[\"backpropagational network\"], [\"color co-occurrence method\"], [\"canonical discriminant classification\"], [\"convolutional neural network\"], [\"discriminant analysis\"], [\"double hough transform\"], [\"decision tree\"], [\"discrete wavelet transform\"], [\"edge of histogram\"], [\"fast fourier transform\"], [\"fast image processing\"], [\"fisher linear discriminant analysis\"], [\"genetic algorithms\"], [\"generative adversarial network\"], [\"gray level co-occurrence matrix\"], [\"gaussian mixture model\"], [\"hue, intensity, saturation\"], [\"hough transform\"], [\"intersection over union\"], [\"k-nearest neighbor\"], [\"linear binary pattern\"], [\"linear discriminant analysis\"], [\"light detection and ranging\"], [\"linear margin classifier\"], [\"linear regression\"], [\"least square-support vector machine\"], [\"mixer of gaussian\"], [\"machine learning\"], [\"normalised difference vegetation index\"], [\"principal component analysis\"], [\"principal component analysis network\"], [\"probability density functions\"], [\"probabilistic neural network\"], [\"radial basis function\"], [\"robust crop row detection\"], [\"random forest\"], [\"red, green, blue\"], [\"region of interest\"], [\"ratio vegetation index\"], [\"shape matrix histogram\"], [\"self-organizing map\"], [\"sparse principal component analysis\"], [\"support vector data description\"], [\"support vector machine\"], [\"stepwise linear discriminant analysis\"], [\"vegetation indices\"], [\"weed infestation rate\"]]}",
    "{\"title\": \"Summary of studies that employed SVMs for the identification of weeds.\", \"head\": [\"Study\", \"Research problem\", \"Dataset\", \"Accuracy\"], \"value\": [[\"[17]\", \"Detection of weed\", \"20 data points of 9\", \"10-fold cross-\"], [\"\", \"and nitrogen stress in\", \"treatments consisting of\", \"validation used\"], [\"\", \"corn\", \"4 replicates thereby\", \"(testing data set).\"], [\"\", \"\", \"resulting in a data set of\", \"SVM: 66% to 76% for\"], [\"\", \"\", \"720 entries. 50% of the\", \"combined weed and\"], [\"\", \"\", \"data was used for\", \"nitrogen application\"], [\"\", \"\", \"training purposes while\", \"rates.\"], [\"\", \"\", \"the remaining 50% was\", \"73% to 83% accuracy\"], [\"\", \"\", \"used for testing.\", \"83% to 93%\"], [\"\", \"\", \"Hardware used: A\", \"accuracy,\"], [\"\", \"\", \"Compact Airborne\", \"respectively for weed\"], [\"\", \"\", \"Spectrographic Imager\", \"and nitrogen\"], [\"\", \"\", \"\", \"treatments\"], [\"\", \"\", \"\", \"separately.\"], [\"[57]\", \"Classification of\", \"66 color images (30 corn\", \"SVM with different\"], [\"\", \"weed and corn\", \"seedlings, 36 weed\", \"feature selections\"], [\"\", \"seedlings using\", \"images). 60% used for\", \"produced 92.31 to\"], [\"\", \"textural features\", \"training, 40% for testing\", \"100%.\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera\", \"\"], [\"\", \"\", \"(resolution of 640×480\", \"\"], [\"\", \"\", \"pixels).\", \"\"], [\"[42]\", \"Using shape\", \"64 color images (40-\", \"SVM (Sigmoid-\"], [\"\", \"parameters to\", \"training set, 24-testing\", \"96.5%, RBF-67.67%\"], [\"\", \"identify corn/weed\", \"set)\", \"and Polynomial-90%,\"], [\"\", \"seedling in fields\", \"Hardware used: A\", \"respectively)\"], [\"\", \"\", \"digital camera\", \"\"], [\"\", \"\", \"(resolution of 640×480\", \"\"], [\"\", \"\", \"pixels).\", \"\"], [\"[43]\", \"Studying local\", \"200 images (100 each of\", \"SVM: 98.5%\"], [\"\", \"binary pattern for\", \"broadleaf and grass,\", \"\"], [\"\", \"automated weed\", \"respectively). Dataset is\", \"\"], [\"\", \"classification\", \"divided into 10 subsets.\", \"\"], [\"\", \"\", \"1 subset used as the\", \"\"], [\"\", \"\", \"testing set and 9 subsets\", \"\"], [\"\", \"\", \"for training.\", \"\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera\", \"\"], [\"\", \"\", \"(resolution 1200×768\", \"\"], [\"\", \"\", \"pixels)\", \"\"], [\"[44]\", \"Categorize weed\", \"400 features rows for\", \"SVM: True positive\"], [\"\", \"seedlings into groups\", \"training and\", \"true value for all the\"], [\"\", \"for spot spraying and\", \"verification. 240\", \"groups (100%), for\"], [\"\", \"weed scouting\", \"external data sets were\", \"second variant group\"], [\"\", \"\", \"used for testing (100\", \"for Agerantum\"], [\"\", \"\", \"data of amarathus\", \"Conyzoides (66.7%).\"], [\"\", \"\", \"palmer weeds and 100 of\", \"\"], [\"\", \"\", \"other weeds).\", \"\"], [\"\", \"\", \"Weed species:\", \"\"], [\"\", \"\", \"Phylanthus Urinuria,\", \"\"], [\"\", \"\", \"Agerantum Conyzoides\", \"\"], [\"\", \"\", \"sp., Amaranthus palmeri\", \"\"], [\"\", \"\", \"sp., and other weeds\", \"\"], [\"\", \"\", \"(dicotyledon and\", \"\"], [\"\", \"\", \"monocotyledon)\", \"\"], [\"\", \"\", \"Hardware used:\", \"\"], [\"\", \"\", \"Logitech c6I5 Webcam\", \"\"], [\"\", \"\", \"(resolution of\", \"\"], [\"\", \"\", \"1920×1050 pixels)\", \"\"], [\"[56]\", \"Classifying weed\", \"1200 images (500 of\", \"Symlet wavelet\"], [\"\", \"images using\", \"broad category, 500 of\", \"family: 98.1%\"], [\"\", \"Wavelet Transform\", \"narrow category, and\", \"\"], [\"\", \"\", \"200 of unknown\", \"\"], [\"\", \"\", \"category, respectively).\", \"\"], [\"\", \"\", \"Training: 600 images\", \"\"], [\"\", \"\", \"(250 of broad leaves,\", \"\"], [\"\", \"\", \"250 of narrow and 100\", \"\"], [\"\", \"\", \"unknown weeds).\", \"\"], [\"\", \"\", \"Testing: Remaining 600\", \"\"], [\"\", \"\", \"images (250 images of\", \"\"], [\"\", \"\", \"broad leaves, 250 of\", \"\"], [\"\", \"\", \"narrow and 100\", \"\"], [\"\", \"\", \"unknown weeds).\", \"\"], [\"\", \"\", \"Hardware used: Not\", \"\"], [\"\", \"\", \"mentioned\", \"\"], [\"[19]\", \"Classification of\", \"1000 images (500 of\", \"82%\"], [\"\", \"maize and weed\", \"crop, 500 of weed). 450\", \"\"], [\"\", \"\", \"of each were used for\", \"\"], [\"\", \"\", \"Training, 100 for\", \"\"], [\"\", \"\", \"Testing.\", \"\"], [\"\", \"\", \"Hardware used: Not\", \"\"], [\"\", \"\", \"mentioned\", \"\"], [\"[46]\", \"Performance\", \"2560 images. 1155 of\", \"SVM: 100% for crop\"], [\"\", \"comparison of\", \"each class (weed and\", \"and 83.2% for weed\"], [\"\", \"algorithms used for\", \"crop) used for training\", \"\"], [\"\", \"identifying weeds\", \"and 125 images per class\", \"\"], [\"\", \"\", \"used to validate the\", \"\"], [\"\", \"\", \"trained model.\", \"\"], [\"\", \"\", \"Hardware used: A 10\", \"\"], [\"\", \"\", \"MP digital camera\", \"\"]]}",
    "{\"title\": \"Summaries of studies that employed Neural Networks for the identification of weeds.\", \"head\": [\"Study\", \"Research problem\", \"Dataset\", \"Accuracy\"], \"value\": [[\"[54]\", \"A plant classifier\", \"88 corn samples, 77\", \"PNN: 93% accuracy\"], [\"\", \"based on neural\", \"samples of the\", \"for corn and 85% for\"], [\"\", \"networks\", \"buttercup (Ranunculus\", \"weed, Multi-layer\"], [\"\", \"\", \"repens), 79 samples of\", \"Perceptron: 96% for\"], [\"\", \"\", \"Canada thistle (Cirsium\", \"corn and 71% for\"], [\"\", \"\", \"arvense), 75 samples of\", \"weed,\"], [\"\", \"\", \"charlock (Sinapis\", \"SOM: 89% for corn\"], [\"\", \"\", \"arvensis), 73 samples of\", \"and 77% for weed,\"], [\"\", \"\", \"chickweed (Stellaria\", \"Linear Vector\"], [\"\", \"\", \"media), 76 samples of\", \"Quantization: 92% for\"], [\"\", \"\", \"dandelion (Tarraxacum\", \"corn and 84% for\"], [\"\", \"\", \"officinale), 80 samples\", \"weed.\"], [\"\", \"\", \"of grass (Poa annua), 78\", \"\"], [\"\", \"\", \"samples of redshank\", \"\"], [\"\", \"\", \"(Poligonum persicaria),\", \"\"], [\"\", \"\", \"75 samples of stinging\", \"\"], [\"\", \"\", \"nettle (Urtica dioica),\", \"\"], [\"\", \"\", \"78 samples of wood\", \"\"], [\"\", \"\", \"sorrel (Onalis\", \"\"], [\"\", \"\", \"europaea) and 75\", \"\"], [\"\", \"\", \"samples of yellow\", \"\"], [\"\", \"\", \"trefoil (Medicago\", \"\"], [\"\", \"\", \"lupulina)\", \"\"], [\"\", \"\", \"Hardware used: Not\", \"\"], [\"\", \"\", \"mentioned\", \"\"], [\"[55]\", \"Identifying weeds in\", \"1736 color images of\", \"ANN gave accuracies\"], [\"\", \"corn fields using\", \"corn, 772 of velvetleaf,\", \"of 100% for corn, 92%\"], [\"\", \"ANN\", \"672 of quackgrass, 752\", \"for velvetleaf, 62% for\"], [\"\", \"\", \"of common\", \"quackgrass and 80%\"], [\"\", \"\", \"lambsquarters, and\", \"for yellow nutsedge.\"], [\"\", \"\", \"1480 of yellow\", \"\"], [\"\", \"\", \"nutsedge.\", \"\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera (Kodak\", \"\"], [\"\", \"\", \"DC50)\", \"\"], [\"[74]\", \"Using textural\", \"66 color images (30\", \"SVM with different\"], [\"\", \"features for\", \"corn seedlings, 36 weed\", \"feature selections\"], [\"\", \"classification of\", \"images). 60% used for\", \"produced 92.31 to\"], [\"\", \"weeds corn\", \"training, 40% for\", \"100%.\"], [\"\", \"\", \"testing\", \"BP: 80%\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera\", \"\"], [\"\", \"\", \"(resolution of 640×480\", \"\"], [\"\", \"\", \"pixels).\", \"\"], [\"[56]\", \"Identifying weed or\", \"35 images of corn and\", \"BP network with seven\"], [\"\", \"corn using wavelet\", \"49 of weed. Training:\", \"wavelet energy\"], [\"\", \"features and fractal\", \"49 images (20 images\", \"parameters: 77.14%\"], [\"\", \"dimension\", \"of corn and 29 images\", \"BP network with\"], [\"\", \"\", \"of weed). Testing: 35\", \"wavelet energy\"], [\"\", \"\", \"images (15 images of\", \"parameters and fractal\"], [\"\", \"\", \"corn and 20 images of\", \"dimension as input:\"], [\"\", \"\", \"weed).\", \"94.28%.\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera with a\", \"\"], [\"\", \"\", \"resolution of 640×480\", \"\"], [\"\", \"\", \"pixels\", \"\"], [\"[42]\", \"Using shape\", \"200 images (100 each\", \"SVM (96.5%, 67.67%\"], [\"\", \"parameters to\", \"of broadleaf and grass,\", \"and 90%,\"], [\"\", \"identify single corn\", \"respectively). Dataset\", \"respectively), ANN\"], [\"\", \"or weed seedlings in\", \"was divided into 10\", \"(83.2%)\"], [\"\", \"fields\", \"subsets. 1 subset used\", \"\"], [\"\", \"\", \"as the testing set 9\", \"\"], [\"\", \"\", \"subsets for training.\", \"\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera\", \"\"], [\"\", \"\", \"(resolution 1200×768\", \"\"], [\"\", \"\", \"pixels)\", \"\"], [\"[57]\", \"Identification of\", \"60 color images of corn\", \"PNN: 92.5%\"], [\"\", \"weeds in corn\", \"and 300 of weed.\", \"recognition rate for\"], [\"\", \"seedlings field\", \"Training: 120 (20 of\", \"corn seedlings and\"], [\"\", \"\", \"corn and 100 of weed).\", \"95% recognition rate\"], [\"\", \"\", \"Testing: 240 (40 and\", \"for weeds.\"], [\"\", \"\", \"200 of corn and weed,\", \"\"], [\"\", \"\", \"respectively)\", \"\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera with a\", \"\"], [\"\", \"\", \"resolution of 640×480\", \"\"], [\"\", \"\", \"pixels\", \"\"], [\"[58]\", \"Wavelet-based crop\", \"20 images of corn and\", \"98.8% classification\"], [\"\", \"detection and\", \"30 of weeds (all\", \"accuracy.\"], [\"\", \"classification\", \"vegetation without\", \"\"], [\"\", \"\", \"corn) were used to\", \"\"], [\"\", \"\", \"build the ANN model.\", \"\"], [\"\", \"\", \"15 images of corns and\", \"\"], [\"\", \"\", \"20 of weeds were used\", \"\"], [\"\", \"\", \"to evaluate it.\", \"\"], [\"\", \"\", \"Hardware used: A\", \"\"], [\"\", \"\", \"digital camera\", \"\"], [\"\", \"\", \"(Canonixus) used to\", \"\"], [\"\", \"\", \"obtain digital images.\", \"\"], [\"[59]\", \"Automated weed\", \"224 aerial images. The\", \"Validation accuracy:\"], [\"\", \"detection in aerial\", \"dataset was divided\", \"97.1% for edge-\"], [\"\", \"imagery\", \"into 3 categories: No\", \"stretched, 94.6% for\"], [\"\", \"\", \"context, Full stretched\", \"no context and 96.3%\"], [\"\", \"\", \"and Edge stretched\", \"for full-stretched.\"], [\"\", \"\", \"data.\", \"\"], [\"\", \"\", \"Hardware used: Sony\", \"\"], [\"\", \"\", \"A6000 mounted on a\", \"\"], [\"\", \"\", \"drone.\", \"\"], [\"[60]\", \"CNN-based pixel-\", \"8340 and 301 images of\", \"First image: IOU was\"], [\"\", \"wise classification of\", \"the segmented plants\", \"0.93, 0.98 and 0.79 for\"], [\"\", \"crop and weeds\", \"and soil, respectively.\", \"crop, soil and weeds,\"], [\"\", \"\", \"80% of the plant images\", \"respectively.\"], [\"\", \"\", \"were used to generate\", \"Second image: IOU\"], [\"\", \"\", \"training data while\", \"was 0.71, 0.93 and\"], [\"\", \"\", \"20% was used to\", \"0.70 for crop, soil and\"], [\"\", \"\", \"generate testing data.\", \"weeds, respectively.\"], [\"\", \"\", \"Hardware used: No\", \"\"], [\"\", \"\", \"hardware used\", \"\"]]}",
    "{\"title\": \"Summaries of studies that employed miscellaneous models for the identification of weeds.\", \"head\": [\"Study\", \"Research problem\", \"Dataset\", \"Accuracy\"], \"value\": [[\"[61]\", \"Detection of weed\", \"20 data points of 9 treatments\", \"DT: 71% for first\"], [\"\", \"and nitrogen\", \"consisting of 4 replicates\", \"classification\"], [\"\", \"stress in corn\", \"thereby resulting in a data set\", \"problem at\"], [\"\", \"\", \"of 720 entries. 50% of the data\", \"tasseling stage\"], [\"\", \"\", \"was used for training purposes\", \"DA: 79% for third\"], [\"\", \"\", \"while the remaining 50% was\", \"classification\"], [\"\", \"\", \"used for testing.\", \"problem at full\"], [\"\", \"\", \"Hardware used: A Compact\", \"growth stage\"], [\"\", \"\", \"Airborne Spectrographic\", \"ANN: 71% for first\"], [\"\", \"\", \"Imager (CASI)\", \"classification\"], [\"\", \"\", \"\", \"problem at\"], [\"\", \"\", \"\", \"tasseling stage\"], [\"[62]\", \"Use of FFT to\", \"Hardware used: Robotic\", \"80 corn field\"], [\"\", \"classify weed and\", \"cultivators with a digital\", \"images were used\"], [\"\", \"corn\", \"camera to capture images and\", \"to test\"], [\"\", \"\", \"pre-processing was done to\", \"classification\"], [\"\", \"\", \"obtain RGB images.\", \"accuracy. 5927\"], [\"\", \"\", \"\", \"blocks of size\"], [\"\", \"\", \"\", \"1024×768 were\"], [\"\", \"\", \"\", \"detected as weed,\"], [\"\", \"\", \"\", \"3217 as crop, and\"], [\"\", \"\", \"\", \"8579 correctly\"], [\"\", \"\", \"\", \"classified.\"], [\"\", \"\", \"\", \"Accuracy was\"], [\"\", \"\", \"\", \"92.8%\"], [\"[63]\", \"Discriminating\", \"300 simulated images, 100 in-\", \"Simulated images:\"], [\"\", \"crop and weed in\", \"field images (35 of wheat, 35\", \"100% each for\"], [\"\", \"agronomic images\", \"of sunflower, and 30 of maize)\", \"crops with low\"], [\"\", \"\", \"\", \"WIR, 94% and\"], [\"\", \"\", \"\", \"92%, respectively\"], [\"\", \"\", \"\", \"for medium, 89%\"], [\"\", \"\", \"\", \"and 82%,\"], [\"\", \"\", \"\", \"respectively for\"], [\"\", \"\", \"\", \"high.\"], [\"\", \"\", \"\", \"100 in-field RGB\"], [\"\", \"\", \"\", \"images: 88%.for\"], [\"\", \"\", \"\", \"30 images of\"], [\"\", \"\", \"\", \"maize with low\"], [\"\", \"\", \"\", \"WIR\"], [\"[64]\", \"Discriminate\", \"1530 images\", \"Daubechies 25:\"], [\"\", \"between crop and\", \"Hardware used: A digital\", \"80.7%\"], [\"\", \"weed using\", \"camera (CANON Ixus 330)\", \"Discrete\"], [\"\", \"wavelet transform\", \"\", \"approximation\"], [\"\", \"\", \"\", \"Meyer wavelets:\"], [\"\", \"\", \"\", \"80.6%.\"], [\"[65]\", \"Vision-based\", \"No information about data\", \"Error < ± 5 and ±\"], [\"\", \"autonomous weed\", \"Hardware used: Not\", \"10 pixels for\"], [\"\", \"detection\", \"mentioned\", \"translation and\"], [\"\", \"\", \"\", \"degrees for\"], [\"\", \"\", \"\", \"orientation,\"], [\"\", \"\", \"\", \"respectively.\"], [\"[66]\", \"Crop and weeds\", \"No information about data\", \"LDA over PCA:\"], [\"\", \"classification\", \"Hardware used: Not\", \"91.8%\"], [\"\", \"based on their UV-\", \"mentioned\", \"\"], [\"\", \"induced\", \"\", \"\"], [\"\", \"fluorescence\", \"\", \"\"], [\"\", \"spectral signature\", \"\", \"\"], [\"[67]\", \"Discrimination of\", \"6 video segments (each having\", \"Successfully\"], [\"\", \"crop and weed by\", \"an average 12 s duration i.\", \"detecting an\"], [\"\", \"using real-time\", \"e.,300 frames) thereby\", \"average of 95%\"], [\"\", \"image processing\", \"resulting in a total of 1800\", \"for weeds and\"], [\"\", \"\", \"frames\", \"80% of crops\"], [\"\", \"\", \"Hardware used: Sony DCR\", \"under different\"], [\"\", \"\", \"PC110E and JVC GR-DV700E\", \"environmental\"], [\"\", \"\", \"(resolution of 720×576\", \"conditions.\"], [\"\", \"\", \"pixels).\", \"\"], [\"[68]\", \"Recognizing\", \"118 color images. 40 images of\", \"SVDD (eH2, eV2,\"], [\"\", \"images of weed\", \"corn and 10 of weed for used\", \"T): 95.59%\"], [\"\", \"and corn using\", \"for training. 35 images of corn\", \"SVDD (eH2, eV2,\"], [\"\", \"SVDD\", \"and 33 of weed were used for\", \"C, D, T): 95.59%\"], [\"\", \"\", \"testing.\", \"\"], [\"\", \"\", \"Hardware used: Olympus FE-\", \"\"], [\"\", \"\", \"280 digital camera with a\", \"\"], [\"\", \"\", \"resolution of 1280×960\", \"\"], [\"\", \"\", \"pixels.\", \"\"], [\"[22]\", \"Using Bayesian\", \"149 images of crops (corn and\", \"An average of\"], [\"\", \"classification to\", \"soybean) and weeds\", \"94% of corn and\"], [\"\", \"isolate weeds in\", \"Hardware used: Not\", \"soybean plants\"], [\"\", \"row crops\", \"mentioned\", \"were classified\"], [\"\", \"\", \"\", \"and 85% of the\"], [\"\", \"\", \"\", \"weeds were\"], [\"\", \"\", \"\", \"classified\"], [\"[70]\", \"Using LIDAR to\", \"1558 sampling units\", \"CDA overall\"], [\"\", \"discriminate\", \"Hardware used: Not\", \"accuracy: 72.2%.\"], [\"\", \"between maize\", \"mentioned\", \"Accuracy for\"], [\"\", \"and weeds\", \"\", \"dicots: 64.5%, and\"], [\"\", \"\", \"\", \"Accuracy for crop:\"], [\"\", \"\", \"\", \"74.3%.\"], [\"[71]\", \"Automatic\", \"73 images\", \"95.89%. 3 images\"], [\"\", \"classification of\", \"Hardware used: Digital\", \"were misclassified\"], [\"\", \"weeds and corn\", \"camera (normal webcam).\", \"\"], [\"\", \"\", \"RBG images were captured\", \"\"], [\"\", \"\", \"with a size of 640×480 pixels\", \"\"], [\"[69]\", \"Detection of crop\", \"60 RGB images\", \"91.67%, 5 images\"], [\"\", \"row and\", \"Hardware used: Not\", \"misclassified\"], [\"\", \"distinguishing\", \"mentioned\", \"\"], [\"\", \"between crop and\", \"\", \"\"], [\"\", \"weed in field with\", \"\", \"\"], [\"\", \"high weed\", \"\", \"\"], [\"\", \"pressure\", \"\", \"\"], [\"[75]\", \"Recognizing and\", \"110 samples derived through\", \"100% for both\"], [\"\", \"discriminating\", \"feature selection from 110\", \"MOG and SOM.\"], [\"\", \"between weed and\", \"spectra pertaining to maize\", \"MOG based one-\"], [\"\", \"maize using\", \"plants. The one-class\", \"class classifier:\"], [\"\", \"hyperspectral\", \"classifiers were tested to\", \"between 31% to\"], [\"\", \"imaging\", \"recognize the new species as\", \"98%.\"], [\"\", \"\", \"outliers by using 54 additional\", \"SOM based one-\"], [\"\", \"\", \"samples of maize plants and 54\", \"class classifier:\"], [\"\", \"\", \"from a single weed species.\", \"between 53% to\"], [\"\", \"\", \"Type of imaging:\", \"94%.\"], [\"\", \"\", \"Hyperspectral\", \"\"], [\"\", \"\", \"Hardware used: Inspector V9,\", \"\"], [\"\", \"\", \"a 10-bit integration charge-\", \"\"], [\"\", \"\", \"coupled device\", \"\"], [\"[74]\", \"Texture, space\", \"No information about data\", \"Over 95%\"], [\"\", \"and spectral\", \"Hardware used: Field\", \"\"], [\"\", \"features-based\", \"Imaging Spectrometer System\", \"\"], [\"\", \"classification of\", \"(FISS), CCD camera\", \"\"], [\"\", \"weeds and corn\", \"\", \"\"], [\"[75]\", \"Hyperspectral\", \"Maize: 25 images, 24 each for\", \"Maize (94%\"], [\"\", \"imaging based\", \"C. arvensis, C. arvense and\", \"precision and\"], [\"\", \"classification of\", \"Rumex.\", \"100% recall),\"], [\"\", \"maize and weed\", \"Type of imaging:\", \"Rumex (70.3%\"], [\"\", \"\", \"Hyperspectral\", \"precision), C.\"], [\"\", \"\", \"Hardware used: Snapshot\", \"arvensis (95.9%\"], [\"\", \"\", \"mosaic hyperspectral camera\", \"precision) and C.\"], [\"\", \"\", \"and sensor.4 OSRAM Halogen\", \"arvense (65.9%\"], [\"\", \"\", \"lamps\", \"precision).\"], [\"[76]\", \"Identification of\", \"Training data: 197 RGB image\", \"Accuracies of LS-\"], [\"\", \"weed and using\", \"samples from 2011, 4333\", \"SVM for three\"], [\"\", \"color indices\", \"samples of maize, 5730 of\", \"years: 89.08%,\"], [\"\", \"\", \"weed from 2011, 3573\", \"87.87% and\"], [\"\", \"\", \"samples of maize and 7976 of\", \"90.44%\"], [\"\", \"\", \"weed from 2012\", \"Accuracies of\"], [\"\", \"\", \"Testing data: 1465 samples of\", \"SVDD for three\"], [\"\", \"\", \"maize and 7878 of weed from\", \"years: 90.19%,\"], [\"\", \"\", \"2013 were\", \"92.36% and\"], [\"\", \"\", \"Hardware used: E450\", \"93.87%.\"], [\"\", \"\", \"Olympus\", \"\"], [\"\", \"\", \"(resolution = 3648 × 2736\", \"\"], [\"\", \"\", \"pixels, focal length = 16 mm).\", \"\"]]}",
    "{\"title\": \"Definitions of a classifier predictions.\", \"head\": [\"Terminology\", \"\", \"Definition\"], \"value\": [[\"True Positive\", \"TP\", \"Correctly identified an instance to a particular class\"], [\"False Positive\", \"FP\", \"Incorrectly identified an instance to a particular class\"], [\"True\", \"TN\", \"Correctly identified an instance does not belong to a\"], [\"Negative\", \"\", \"particular class\"], [\"False\", \"FN\", \"Incorrectly identified an instance does not belong to a\"], [\"Negative\", \"\", \"particular class\"]]}",
    "{\"title\": \"Mathematical definitions of performance metrics. Note: pprobability of an instance belonging to a class.\", \"head\": [\"Performance metric\", \"Definition\"], \"value\": [[\"Precision (P)\", \"TP/(TP+FP)\"], [\"Recall (R) / True Positive Rate (TPR)\", \"TP/(TP+FN)\"], [\"Specificity (S) / True Negative Rate (TNR)\", \"TN/(TN+FP)\"], [\"Miss Rate / False Positive Rate (FPR)\", \"FN/(TP+FN)\"], [\"F1 Score\", \"2PR/(P + R)\"], [\"Log Loss (for one-hot coded vectors)\", \"-log p\"]]}"
  ]
}